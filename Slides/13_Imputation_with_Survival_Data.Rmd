---
title: "EP16: Missing Values in Clinical Research: Multiple Imputation"
subtitle: "13. Imputation of Survival Data"
author: "Nicole Erler"
institute: "Department of Biostatistics, Erasmus Medical Center"
date: ""
email: "n.erler@erasmusmc.nl"
output:
  beamer_presentation:
    keep_tex: false
    template: mytemplate.latex
    includes:
      in_header: [SlideTemplate.tex, defs.tex]
    incremental: false
classoption: [aspectratio=169]
bibliography: references.bib
---

```{r setup, include = FALSE}
projdir <- gsub("/Slides", "", getwd())

runimps <- FALSE
  
knitr::knit_hooks$set(
  nospace = function(before, options, envir) {
    if (before) {
      knitr::asis_output("\\vspace*{-1.5ex}")
    }
  }
)


knitr::opts_chunk$set(echo = TRUE, nospace = TRUE, nospaceafter = TRUE,
                      fig.align = 'center', out.width = "100%")

options(width = 120)

suppressPackageStartupMessages(library("mice"))
suppressPackageStartupMessages(library("JointAI"))

library(kableExtra)
library(ggplot2)

load(file.path(projdir, "Slides/workspaces/survdat.RData"))
load(file.path(projdir, 'Slides/workspaces/JointAI_surv.RData'))

impsurv_naive <- mice(survdat, printFlag = FALSE)
```


## Results from the Literature
In a previous section we saw that the correct
conditional distribution for an incomplete covariate $x$ in a proportional hazards model is
rather complex:
$$\log p(x\mid T, D, z) = \log p(x\mid z) + D(\beta_x x + \beta_z z) -
H_0(T)\exp(\beta_x x+\beta_z z) + const.$$

\bigskip

@White2009 investigated how to best approximate this formula in multiple imputation:

\bigskip

\blue{\ding{225}} Use $Z$, $D$ and $H_0(T)$, and possibly an interaction term 
as predictor variables.

This often works satisfactorily \blue{if covariate effects and cumulative incidences are rather small}.


## Results from the Literature
\blue{Problem:} in practice $H_0(T)$ is unspecified.

Two main ideas:
\begin{itemize}\itemsep1ex
\item If covariate effects $\beta_x$ and $\beta_z$ are small: \blue{$H_0(t)\approx H(t)$}\\
      \blue{\ding{225}} $H(t)$ can be approximated by the \blue{Nelson-Aalen estimator}.
\item \blue{Estimate $H_0(T)$ in an additional step} inside MICE\\
       \blue{\ding{225}} fit a Cox model on the imputed data in each iteration
\end{itemize}

\pause
\blue{Conclusion} [@White2009]:\
Use \blue{$Z$, $D$ and the Nelson-Aalen estimator $\hat H(T)$} as predictors
for the imputation of $X$.



## Results from the Literature
\blue{Note:}

* Neither of these approaches takes into account uncertainty about $H_0(t)$
  (but the impact is likely to be small).
  
* Using the Nelson-Aalen estimator is an approximation\
  \blue{\ding{225}} some \blue{bias towards the null} should be expected when covariates
  have large effects.


## Imputation with mice
**Example Data:**
\small
```{r}
head(survdat)
```

Calculate the Nelson-Aalen estimator using `nelsonaalen()`{.R} from the package
**mice**:
\small
```{r mice_surv01}
survdat$H0 <- nelsonaalen(survdat, timevar = Time, statusvar = event)
```
\normalsize


## Imputation with mice
\small
```{r mice_surv02}
# setup run
imp0 <- mice(survdat, maxit = 0)
meth <- imp0$method
pred <- imp0$predictorMatrix

# specify normal imputation for continuous covariates
meth[c("x1", "x3")] <- "norm"

# remove event time from predictor (high correlation with H0)
pred[, "Time"] <- 0
```
```{r, eval = FALSE}
pred
```
\scriptsize
```{r, echo = FALSE}
pred
```


## Imputation with mice
```{r mice_surv03}
# run the imputation
survimp <- mice(survdat, maxit = 10, method = meth,
                predictorMatrix = pred, printFlag = FALSE)
```

To obtain the pooled results, we first fit the model of interest
\small
```{r mice_surv2}
library("survival")
cox_mice <- with(survimp, coxph(Surv(Time, event) ~ x1 + x2 + x3))
```

and pool and summarize the results.
\small
```{r, warning = FALSE}
res_mice_surv <- summary(pool(cox_mice), conf.int = TRUE)
```


## Imputation with JointAI
Two options:

* `coxph_imp()`{.R}\
   proportional hazards model with flexible baseline hazard
* `survreg_imp()`{.R}\
   parametric (Weibull) model (AFT model)
   
\pause\bigskip
\small
```{r, eval = runimps}
JointAI_cox <- coxph_imp(Surv(Time, event) ~ x1 + x2 + x3, data = survdat,
                          n.iter = 1500)

JointAI_survreg <- survreg_imp(Surv(Time, event) ~ x1 + x2 + x3,
                               data = survdat, n.iter = 1500)
```

```{r, eval = runimps, echo = FALSE}
save(JointAI_cox, JointAI_survreg, file = 'Slides/workspaces/JointAI_surv.RData')
```


## Comparison of the Results
\small
```{r, echo = F, fig.width = 6, fig.height = 2.5}
mod_cox <- with(survdat_orig, coxph(Surv(Time, event) ~ x1 + x2 + x3))
mod_survreg <- with(survdat_orig, survreg(Surv(Time, event) ~ x1 + x2 + x3))

mice_wb <- with(survimp, survreg(Surv(Time, event) ~  x1 + x2 + x3))
mice_naive_wb <- with(impsurv_naive, survreg(Surv(Time, event) ~  x1 + x2 + x3))
mice_naive_cox <- with(impsurv_naive, coxph(Surv(Time, event) ~  x1 + x2 + x3))


res_mice_wb <- summary(pool(mice_wb, dfcom = 99999),
                       conf.int = TRUE)[, c("term", "estimate", "2.5 %", "97.5 %")]

res_mice_naive_wb <- summary(pool(mice_naive_wb, dfcom = 99999),
                             conf.int = TRUE)[, c("term", "estimate", "2.5 %", "97.5 %")]

res_mice_naive_cox <- summary(pool(mice_naive_cox, dfcom = 99999),
                             conf.int = TRUE)[, c("term", "estimate", "2.5 %", "97.5 %")]


res_survreg <- list(mice_naive = res_mice_naive_wb,
                    mice = res_mice_wb,
                    JointAI = cbind(
                      term = c("(Intercept)", "x1", "x21", "x3"), 
                      as.data.frame(
                        summary(JointAI_survreg)$res[[1]]$regcoef[
                        c("(Intercept)", "x1", "x21", "x3"), c(1,3,4)]))
)

res_cox <- list(mice_naive = res_mice_naive_cox,
                mice = res_mice_surv[, c("term", "estimate", "2.5 %", "97.5 %")],
                JointAI = cbind(
                  term = c("x1", "x21", "x3"),
                  as.data.frame(summary(JointAI_cox)$res[[1]]$regcoef[
                    c("x1", "x21", "x3"), c(1,3,4)]))
)


res_survreg <- lapply(res_survreg, function(x) {
  colnames(x) <- gsub("5%", "5 %", colnames(x)) %>%
    gsub("Mean", "estimate", .)
  x#[x$term %in% names(coef(JointAI_survreg)),]
})


res_cox <- lapply(res_cox, function(x) {
  colnames(x) <- gsub("5%", "5 %", colnames(x)) %>%
    gsub("Mean", "estimate", .)
  x#[x$term %in% names(coef(JointAI_cox)),]
})

plot_survreg <- reshape2::melt(res_survreg, id.vars = names(res_survreg[[1]]))
plot_cox <- reshape2::melt(res_cox, id.vars = names(res_cox[[1]]))

polyDF_cox <- data.frame(x = rep(c(0.5, length(res_cox) + .5)[c(1,2,2,1)],
                                 length(mod_cox$coef)),
                         y = c(apply(log(summary(mod_cox)$conf.int[, 3:4]), 1, rep,
                                     each = 2)),
                         term = rep(c("x1", "x21", "x3"), each = 4)
)


polyDF_survreg <- data.frame(x = rep(c(0.5, length(res_survreg) + .5)[c(1,2,2,1)],
                                 length(mod_survreg$coef)),
                             y = c(apply(confint(mod_survreg), 1, rep,
                                     each = 2)),
                         term = rep(c("(Intercept)", "X1", "X21","X3"), each = 4)
)


ggplot(plot_cox, aes(x = L1, y = estimate)) +
  geom_point(na.rm = T) +
  geom_errorbar(aes(ymin = `2.5 %`, ymax = `97.5 %`), width = 0.3, na.rm = T) +
  facet_wrap("term", scales = 'free', nrow = 1) +
  geom_polygon(data = polyDF_cox, aes(x = x, y = y), fill = 'blue', alpha = 0.2) +
  geom_hline(data = data.frame(betas = coef(mod_cox),
                               term = names(coef(mod_cox))),
             aes(yintercept = betas), lty = 2) +
  ylab("coefficient") +
  xlab("") +
   theme(panel.background = element_rect(fill = grey(0.98), color = grey(0.85)),
          panel.grid = element_blank(),
          legend.position = 'none',
          legend.key.width = unit(10, "mm"),
          legend.key.height = unit(10, "mm"),
         axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_x_discrete(limits = c("mice_naive", "mice", "JointAI"),
                   labels = c("mice\nnaive", "mice", "JointAI"))


# 
# ggplot(plot_survreg, aes(x = L1, y = estimate)) +
#   geom_point(na.rm = T) +
#   geom_errorbar(aes(ymin = `2.5 %`, ymax = `97.5 %`), width = 0.3, na.rm = T) +
#   facet_wrap("term", scales = 'free', nrow = 1) +
#   geom_polygon(data = polyDF_survreg, aes(x = x, y = y), fill = 'blue', alpha = 0.2) +
#   geom_hline(data = data.frame(betas = coef(mod_survreg),
#                                term = names(coef(mod_survreg))),
#              aes(yintercept = betas), lty = 2) +
#   ylab("coefficient") +
#   xlab("") +
#   theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
#   scale_x_discrete(limits = c("mice_naive", "mice", "JointAI"),
#                    labels = c("mice\nnaive", "mice", "JointAI"))

```

\normalsize

Note that the \blue{true effects} (log HR) of $x1$ and $x2$ are \blue{very large}
(-2 and 2.5, respectively),
and represent the setting where the approximation by the Nelson-Aalen estimate
is \blue{expected to be biased}.


## References
