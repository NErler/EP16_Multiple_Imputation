\begin{frame}{Outline of Part I}
\setlength{\parskip}{0pt}
\begin{multicols}{2}
\tableofcontents[part=1]
\vspace*{16ex}
\end{multicols}
\end{frame}


% Section 1: Multiple Imputation theory ----------------------------------------
\section{What is Multiple Imputation?}\label{sec:Sec1}
\subsection{History \& Ideas}\label{subsec:history}
\begin{frame}[fragile, label=history]{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
% \framesubtitle{What is Multiple Imputation?}
\begin{itemize}
\item Developed by \blue{Donald B. Rubin} in the 1970s,
\item to handle missing values in \blue{public use databases},\\
      e.g., census data provided by the government,
\item motivated by the \blue{increase in missing values}, and
\item increased \blue{availability of computers}.
\end{itemize}

\bigskip

\pause
Such data should be usable by \cite{Rubin1996}
\begin{itemize}
\item a \blue{large number of analysts}, who commonly have to rely on
\item standard \blue{software that can only handle complete data}, and usually
\item are \blue{not experts in handling incomplete data}.
\end{itemize}

% \bigskip
%
% \pause
% Rubin's thoughts:\cite{Rubin2004}
% \begin{enumerate}[(I)]
% \item one imputed value can not be correct in general\\
%       \blue{\ding{225}} we need to represent missing values by a \blue{number of imputations}
% \item to find \blue{sensible values} to fill in, we need some kind of \blue{model}
% \end{enumerate}
\end{frame}


\begin{frame}[fragile, label = RubinsIdeas]{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
\blue{Rubin's thoughts:} \cite{Rubin2004}
\begin{columns}[T, onlytextwidth]
  \begin{column}{0.47\textwidth}
    \begin{block}{}
      One imputed value can not be correct in general.\\
      \blue{\ding{225}} We need to represent missing values by a
      \blue{number of imputations}.
    \end{block}
    \vspace*{-2ex}
    \onslide<2->{\centering\scalebox{3}{\rotatebox{-90}{\blue{\ding{225}}}}}
  \end{column}
  \begin{column}{0.1\textwidth}
  \end{column}
  \begin{column}{0.4\textwidth}
    \begin{block}{}
      To find \blue{sensible values} to fill in, we need some kind of \blue{model}.\\
    \end{block}
    \vspace*{-2ex}
    \onslide<2->{\centering\scalebox{3}{\rotatebox{-90}{\blue{\ding{225}}}}}
  \end{column}
\end{columns}

\vspace*{-3ex}

\pause
\begin{columns}[onlytextwidth]
  \begin{column}{0.47\textwidth}
    \begin{block}{}
      \blue{Missing data has a distribution.}
    \end{block}
  \end{column}
  \begin{column}{0.1\textwidth}
    \centering\raisebox{-3ex}[0ex][0ex]{\scalebox{3}{\blue{\ding{225}}}}
  \end{column}
  \begin{column}{0.4\textwidth}
    \begin{block}{}
      This \blue{distribution depends on assumptions} that have been made about the model.
    \end{block}
  \end{column}
\end{columns}

\vspace*{1ex}

\pause
\begin{columns}[onlytextwidth]
  \begin{column}{0.47\textwidth}
  \end{column}
  \begin{column}{0.1\textwidth}
  \end{column}
  \begin{column}{0.4\textwidth}
    \centering\scalebox{3}{\rotatebox{-90}{\blue{\ding{225}}}}
  \end{column}
\end{columns}

\begin{block}{}
What we want to impute is the\\
\blue{`predictive distribution' of the missing values given the observed values.}
\end{block}
\end{frame}


% \begin{frame}[fragile]{\thesection. \insertsection}
% \framesubtitle{\thesection.\thesubsection. \insertsubsection}
% This focus on imputed values (and not estimated parameters) is a Bayesian notion.
% Following the Bayesian context: \cite{Rubin2004}
% \begin{quote}
% \begin{enumerate}[(I)]
% \item The \blue{missing data has a distribution} given the observed data
%       (the predictive distribution).
% \item This \blue{distribution depends on assumptions} that have been made about the model.
% \end{enumerate}
% \end{quote}
%
% \bigskip
%
% \blue{\ding{225}} What we really want to impute is the \blue{`predictive distribution' of the missing
% values given the observed values}\ldots \cite{Rubin2004}
%
% \end{frame}


\begin{frame}[fragile]{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
\textbf{How to obtain that predictive distribution?}\\

Rubin suggests to
\begin{itemize}
\item fit a model to the observed data (``respondents''), and to
\item obtain for each ``nonrespondent'' the conditional distribution of the
      missing data (given the observed data) as if he/she was a respondent.
\end{itemize}

\blue{\ding{225}} We assume nonrespondents are just like respondents, and
obtain the predictive distribution from the model of the respondents data.

\vfill

\pause
\begin{block}{\textbf{Example:} survey about age, gender and height}
Boys aged 10 -- 12 years old answered (on average) that they are 1.45m tall.\\
\ding{225} \parbox[t]{\dimexpr\linewidth-5em}{
  We assume that boys aged 10 to 12 who did not
  report their height are also around 1.45m tall.}
\end{block}
\end{frame}
% \begin{quote}
% (1) model the respondents data\\
% (2) obtain for each nonrespondent the conditioanal distribution of missing data
% (given observed data) as if he were a respondent (i.e., the predictive
% distribution under the model that says each nosnrespondent is just like a
% respondent with the same values of observed variables)\\
% (3) alter theis distribution in various ways to alow for nonresponse bias
% \end{quote}


\begin{frame}[fragile]{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
\textbf{How to represent the multiple imputed values?}\label{slide:RubinI}\\
For each missing value, we now have multiple imputed values.
\begin{itemize}
\item For each set of imputed values, create a dataset\\
% \item generate many datasets from the predictive distribution\\
      (those datasets agree in the observed values but imputed values differ).
\item Analyse each dataset, and
\item take the results from each analysis.
\end{itemize}

\bigskip

\blue{\ding{225}} We can describe how (much) the \blue{results vary between the
imputed datasets}, and calculate summary measures.
% Summarize the parameters from the separate analyses
% to describe how the conclusions change under different imputed values.
% \begin{quote}
% ``\ldots generate many datasets drawn from the predictive distribution of the
% missing values, given the observed values. All these datasets agree for the
% observed data values; the values filled in for the missing values differ from
% dataset to dataset.'' (Rubin, 1977)\cite{Rubin2004}
% \end{quote}
% \end{frame}
\end{frame}


\subsection{Three steps}
\begin{frame}[fragile]{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
\begin{center}\vspace*{-3ex}
\includegraphics[height = 0.6\textheight]{graphics/MI.pdf}
\end{center}\vspace*{-3ex}
\begin{block}{In summary:}
\begin{enumerate}
\item \blue{Imputation:} impute multiple times \blue{\ding{225}} multiple completed datasets
\item \blue{Analysis:} analyse each of the datasets
\item \blue{Pooling:} combine results, taking into account additional uncertainty
\end{enumerate}
\end{block}
\end{frame}




\section{Imputation step}
\subsection{Univariate missing data}
<<simdata1, echo = F, message = F>>=
library(MASS)
library(mice)
library(RColorBrewer)

set.seed(123)
p = 2
N = 30
mu <- rnorm(p)
# s <- rgamma(p*(p - 1)/2, 0.9, 8)
s <- 0.1
S <- diag(rgamma(p, 0.5, 1.5))
S[lower.tri(S)] <- s
S[upper.tri(S)] <- s
S <- Matrix::nearPD(S)$mat
X0 <- as.data.frame(mvrnorm(N, mu, S))

X <- X0
X$V2[sample.int(N, N/5)] <- NA


m <- 5
imps <- array(dim = c(sum(is.na(X$V2)), 4, m),
               dimnames = list(sim = c(),
                               method = c("pred", "nob", "norm", "boot"),
                               c()))

imps[, "pred", ] <- mice.impute.norm.predict(y = X$V2, ry = !is.na(X$V2), wy = is.na(X$V2),
                         x = model.matrix(~V1, X), ridge = 1e-10)
for (i in 1:m) {
  imps[, "nob", i] <- mice.impute.norm.nob(y = X$V2, ry = !is.na(X$V2), wy = is.na(X$V2),
                                 x = model.matrix(~V1, X))
  imps[, "norm", i] <- mice.impute.norm(y = X$V2, ry = !is.na(X$V2), wy = is.na(X$V2),
                                  x = model.matrix(~V1, X))
  imps[, "boot", i] <- mice.impute.norm.boot(y = X$V2, ry = !is.na(X$V2), wy = is.na(X$V2),
                                  x = model.matrix(~V1, X))
}

lm1 <- lm(V2 ~ ., X)
x <- X$V1[is.na(X$V2)]


sim1DF <- data.frame(x1 = X[, 1],
                     x2 = X[, 2],
                     ismis = is.na(X[, 2]))
sim1DF$x2[sim1DF$ismis] <- -2.5


set.seed(2018)
add <- MASS::mvrnorm(5, c(0,0), vcov(lm1))

coefs <- as.data.frame(t(coef(lm1) + t(add)))
names(coefs) <- c("int", 'slope')

reddots <- reshape2::melt(
  as.data.frame(cbind(x1 = sim1DF$x1[sim1DF$ismis],
                      t(as.matrix(coefs) %*% rbind(1, sim1DF$x1[sim1DF$ismis])))
  ), id.var = "x1", value.name = 'x2')


library(ggplot2)
p0 <- ggplot(sim1DF, aes(x = x1, y = x2, color = ismis)) +
  theme_light() +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.ticks = element_blank(),
        axis.text = element_blank(),
        axis.title = element_text(size = 16),
        panel.border = element_blank(),
        legend.position = c(0.12, 0.13),
        legend.background = element_blank(),
        legend.text = element_text(size = 14)) +
  geom_rect(xmin = -3.3, xmax = 1.8, ymin = -2.5, ymax = 1.5,
            fill = 'transparent', color = grey(0.7)) +
  scale_y_continuous(limits = c(-2.57, 1.5), expand = c(0, 0)) +
  scale_x_continuous(limits = c(-3.3, 1.8), expand = c(0, 0)) +
  xlab(expression(X[1])) +
  ylab(expression(X[2])) +
  scale_color_manual(name = "",
                     limits = c(FALSE, TRUE),
                     values = c(1, 2),
                     labels = c("observed", "missing")) +
  scale_shape_manual(name = "",
                     limits = c(FALSE, TRUE),
                     values = c(19, 1),
                     labels = c("observed", "missing"))


p1 <- p0 +
  geom_point(aes(shape = ismis), na.rm = T, size = 2, stroke = 1.2)

pmod <- p1 +
  geom_abline(intercept = coef(lm1)[1], slope = coef(lm1)[2], lwd = 1)
# pmod

# pmod +
#   geom_point(data = data.frame(x1 = sim1DF$x1[sim1DF$ismis],
#                                x2 = imps[, "pred", 1]),
#              color = 2, shape = 19, size = 2
#              ) +
#   geom_line(data = data.frame(x1 = rep(sim1DF$x1[sim1DF$ismis], 2),
#                               x2 = c(rep(-2.5, sum(sim1DF$ismis)), imps[, "pred", 1]),
#                               group = rep(1:sum(sim1DF$ismis), 2)),
#             aes(group = group), color = 2, lty = 2)
#
# p1 +
#   geom_abline(data = coefs, aes(intercept = int, slope = slope), lty = 2)
#
#
# p1 +
#   geom_abline(data = coefs, aes(intercept = int, slope = slope), lty = 2) +
#   geom_point(data = reddots, color = 2, size = 2)


@


\begin{frame}{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
\textbf{How can we actually get imputed values?}
\begin{columns}[onlytextwidth]
\begin{column}{0.65\textwidth}
For now: assume only one continuous variable has missing values (\blue{univariate missing data})
\end{column}
%
\begin{column}{0.35\textwidth}
\begin{center}
\scalebox{0.8}{
  \begin{tabular}{ccccc}
  $X_1$ & $X_2$ & $X_3$ & $X_4$\\\hline
  \checkmark & NA         & \checkmark & \checkmark\\
  \checkmark & \checkmark & \checkmark & \checkmark\\
  \checkmark & NA         & \checkmark & \checkmark\\
  \vdots     & \vdots     & \vdots     & \vdots
 \end{tabular}
}
\end{center}
\end{column}
\end{columns}

\vfill

\begin{columns}[onlytextwidth]
\begin{column}{0.55\textwidth}
\onslide<2->{%
\blue{Idea:} Predict values\\[2ex]

Model:\\
$x_{i2} = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i3} + \beta_3 x_{i4} + \varepsilon_i$\\[2ex]
}

\onslide<3->{
Imputed/predicted value:\\
$\hat x_{i2} = \hat\beta_0 + \hat\beta_1 x_{i1} + \hat\beta_2 x_{i3} + \hat\beta_3 x_{i4}$
}
\end{column}
\begin{column}{0.45\textwidth}
\only<1>{
<<exampl1a, echo = F, fig.width = 5.25, fig.height = 3.5>>=
par(mgp = c(1, 0.6, 0), mar = c(2, 2.5,0.5, 0.5))
plot(X0, xaxt = "n", yaxt = "n",
     ylab = "",
     xlab = "", bty = "n",
     xaxt = "n", yaxt = "n", cex.lab = 1.5, type = "n")
@
}
\only<2>{
<<exampl1b, echo = F, fig.width = 5.25, fig.height = 3.5>>=
par(mgp = c(1, 0.6, 0), mar = c(2, 2.5,0.5, 0.5))
plot(X,
     pch = 19,
     col = grey(0.2),
     ylab = expression(X[2]),
     xlab = expression(X[1]),
     xaxt = "n", yaxt = "n", cex.lab = 1.5,
     ylim = c(range(X0$V2, 0)))
points(X0$V1[is.na(X$V2)], rep(par("usr")[3], sum(is.na(X$V2))),
       xpd = T, col = 2, lwd = 1.5)
legend("bottomleft", pch = c(1,19), legend = c("missing", "observed"),
       bty = "n", col = c("red", grey(0.2)), cex = 1.3)
abline(lm1, lwd = 2, col = grey(0.2))
@
}\only<3>{
<<example1c, echo = F, fig.width = 5.25, fig.height = 3.5>>=
par(mgp = c(1, 0.6, 0), mar = c(2, 2.5,0.5, 0.5))
plot(X,
     pch = 19,
     col = grey(0.2),
     ylab = expression(X[2]),
     xlab = expression(X[1]),
     xaxt = "n", yaxt = "n", cex.lab = 1.5,
     ylim = c(range(X0$V2, 0)))
points(X0$V1[is.na(X$V2)], rep(par("usr")[3], sum(is.na(X$V2))),
       xpd = T, col = 2, lwd = 1.5)
legend("bottomleft", pch = c(1,19), legend = c("missing", "observed"),
       bty = "n", col = c("red", grey(0.2)), cex = 1.3)
abline(lm1, lwd = 2, col = grey(0.2))
points(x, imps[, "pred", 1], pch = 19, col = 2, cex = 1)
for (i in seq_along(which(is.na(X[, 2])))) {
  lines(x = rep(x[i], 2), col = 2,
        y = c(par("usr")[3], imps[i, "pred", 1]), lty = 2)
}

@
}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
\blue{Problem:}
\begin{itemize}
\item We can obtain \blue{only one imputed value} per missing value,
      but we wanted a whole distribution.
\item The predicted values do not take into account the added
      \blue{uncertainty} due to the missing values.
\end{itemize}

\bigskip


\pause

\blue{\ding{225}} We need to take into account \blue{two sources of uncertainty}:
\begin{itemize}
\item The \blue{parameters} are estimated with \blue{uncertainty}\\
      (represented by the std. error).
\item There is \blue{random variation / prediction error}\\
      (variation of the residuals).
\end{itemize}
\end{frame}

\begin{frame}[label=BayesianImputationI]{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}

\textbf{Taking into account uncertainty about the parameters $\bmath \beta$:}\\
We assume that \blue{$\bmath \beta$ has a distribution},
and we can sample realizations
of $\bmath \beta$ from that distribution.


% Following the Bayesian framework, $\boldsymbol{\hat\beta}$ is not assumed to be
% fixed but to have a \blue{distribution $p(\boldsymbol{\hat\beta})$}.
% By sampling realizations of $\boldsymbol{\hat\beta}$ from that distribution,
% the \blue{uncertainty about the regression coefficients} is taken into account.

\vfill

\begin{columns}[onlytextwidth]
\begin{column}{0.5\textwidth}
\onslide<1->{%
When plugging the different realizations of $\bmath \beta$ into the predictive
model, we obtain \blue{slightly different regression lines}.
}

\bigskip

\onslide<2>{
With each set of coefficients, we also get slightly \blue{different predicted values}.
% These samples of $p(\boldsymbol{\hat\beta})$ are then plugged into the
% predictive model
% $$\hat\beta_0 + \hat\beta_1 x_{i1} + \hat\beta_2 x_{i3} + \hat\beta_3 x_{i4},$$
% giving different regression lines.
}
\end{column}
\begin{column}{0.5\textwidth}
\only<-1>{
<<example1d, echo = F, fig.width = 5.25, fig.height = 3.5, eval = T>>=
par(mgp = c(1, 0.6, 0), mar = c(2, 2.5,0.5, 0.5))
plot(X,
     pch = 19,
     col = grey(0.2),
     ylab = expression(X[2]),
     xlab = expression(X[1]),
     xaxt = "n", yaxt = "n", cex.lab = 1.5,
     ylim = c(range(X0$V2, 0)))
points(X0$V1[is.na(X$V2)], rep(par("usr")[3], sum(is.na(X$V2))),
       xpd = T, col = 2, lwd = 1.5)
legend("bottomleft", pch = c(1,19), legend = c("missing", "observed"),
       bty = "n", col = c("red", grey(0.2)), cex = 1.3)
# abline(lm1, lwd = 2, col = grey(0.2))
set.seed(2018)
add <- MASS::mvrnorm(5, c(0,0), vcov(lm1))

for (i in 1:5) {
  coef <- coef(lm1) + add[i, ]
  abline(coef, lty = 2)
}
@

<<echo = FALSE, fig.width = 5.25, fig.height = 3.5, eval = F>>=
p1 +
  geom_abline(data = coefs, aes(intercept = int, slope = slope), lty = 2) +
  geom_point(data = reddots, color = 2, size = 2, stroke = 1.2)

@


}
\only<2->{
<<example1e, echo = F, fig.width = 5.25, fig.height = 3.5>>=
par(mgp = c(1, 0.6, 0), mar = c(2, 2.5,0.5, 0.5))
plot(X,
     pch = 19,
     col = grey(0.2),
     ylab = expression(X[2]),
     xlab = expression(X[1]),
     xaxt = "n", yaxt = "n", cex.lab = 1.5,
     ylim = c(range(X0$V2, 0)))
points(X0$V1[is.na(X$V2)], rep(par("usr")[3], sum(is.na(X$V2))),
       xpd = T, col = 2, lwd = 1.5)
legend("bottomleft", pch = c(1,19), legend = c("missing", "observed"),
       bty = "n", col = c("red", grey(0.2)), cex = 1.3)
set.seed(2018)
add <- MASS::mvrnorm(5, c(0,0), vcov(lm1))

for (i in 1:5) {
  coef <- coef(lm1) + add[i, ]
  abline(coef, lty = 2)
  points(x, coef[1] + coef[2] * x, col = 2, pch = 19)
}

@
}
\end{column}
\end{columns}
\end{frame}

\begin{frame}[label=BayesianImputationII]{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
\textbf{Taking into account the prediction error:}\\
The model does not fit the data perfectly: observations are scattered around the regression lines.

\bigskip

\begin{columns}[onlytextwidth]
\begin{column}{0.5\textwidth}
We assume that the \blue{data have a distribution}, where
\begin{itemize}
\item<1-> the \blue{mean} for each value is given by the \blue{predictive model}, and
\item<2-> the \blue{variance} is determined by  the variance of the residuals $\bmath\varepsilon$.
\end{itemize}
\end{column}
\begin{column}{0.5\textwidth}
\only<1>{
<<example1f, echo = F, fig.width = 5.25, fig.height = 3.5>>=
par(mgp = c(1, 0.6, 0), mar = c(2, 2.5,0.5, 0.5))
plot(X,
     pch = 19,
     col = grey(0.2),
     ylab = expression(X[2]),
     xlab = expression(X[1]),
     xaxt = "n",
     yaxt = "n",
     cex.lab = 1.5,
     ylim = c(range(X0$V2, -2.5, 2.4)))
points(X0$V1[is.na(X$V2)], rep(par("usr")[3], sum(is.na(X$V2))),
       xpd = T, col = 2, lwd = 1.5)
legend("bottomleft", pch = c(1,19), legend = c("missing", "observed"),
       bty = "n", col = c("red", grey(0.2)), cex = 1.3)

set.seed(2018)
add <- MASS::mvrnorm(5, c(0,0), vcov(lm1))

for (i in 1) {
  coef <- coef(lm1) + add[i, ]
  abline(coef, lty = 2, col = brewer.pal(length(x), "Dark2")[i], lwd = 2)
  pt <- cbind(1, x) %*% coef
  points(x, pt, col = brewer.pal(length(x), "Dark2")[i], lwd = 2, cex = 1.3)
}
@
}
\only<2>{
<<example1g, echo = F, fig.width = 5.25, fig.height = 3.5>>=
par(mgp = c(1, 0.6, 0), mar = c(2, 2.5,0.5, 0.5))
plot(X,
     pch = 19,
     col = grey(0.2),
     ylab = expression(X[2]),
     xlab = expression(X[1]),
     xaxt = "n",
     yaxt = "n",
     cex.lab = 1.5,
     ylim = c(range(X0$V2, -2.5, 2.4)))
points(X0$V1[is.na(X$V2)], rep(par("usr")[3], sum(is.na(X$V2))),
       xpd = T, col = 2, lwd = 1.5)
legend("bottomleft", pch = c(1,19), legend = c("missing", "observed"),
       bty = "n", col = c("red", grey(0.2)), cex = 1.3)

set.seed(2018)
add <- MASS::mvrnorm(5, c(0,0), vcov(lm1))

for (i in 1) {
  coef <- coef(lm1) + add[i, ]
  abline(coef, lty = 2, col = brewer.pal(length(x), "Dark2")[i], lwd = 2)
  pt <- cbind(1, x) %*% coef
  points(x, pt, col = brewer.pal(length(x), "Dark2")[i], cex = 1.3)

  lo <- pt - 1.96 * summary(lm1)$sigma
  hi <- pt + 1.96 * summary(lm1)$sigma
  for (k in 1:length(x)) {
    lines(rep(x[k] + 0.01*i, 2), c(lo[k], hi[k]), lty = 2,
          col = brewer.pal(length(x), "Dark2")[i], lwd = 2)
  }
}
@
}
\only<3>{
<<example1h, echo = F, fig.width = 5.25, fig.height = 3.5>>=
par(mgp = c(1, 0.6, 0), mar = c(2, 2.5,0.5, 0.5))
plot(X,
     pch = 19,
     col = grey(0.2),
     ylab = expression(X[2]),
     xlab = expression(X[1]),
     xaxt = "n",
     yaxt = "n",
     cex.lab = 1.5,
     ylim = c(range(X0$V2, -2.5, 2.4)))
points(X0$V1[is.na(X$V2)], rep(par("usr")[3], sum(is.na(X$V2))),
       xpd = T, col = 2, lwd = 1.5)
legend("bottomleft", pch = c(1,19), legend = c("missing", "observed"),
       bty = "n", col = c("red", grey(0.2)), cex = 1.3)
set.seed(2018)
add <- MASS::mvrnorm(5, c(0,0), vcov(lm1))

for (i in 1) {
  coef <- coef(lm1) + add[i, ]
  abline(coef, lty = 2, col = brewer.pal(length(x), "Dark2")[i], lwd = 2)
  pt <- cbind(1, x) %*% coef
  points(x, pt, col = brewer.pal(length(x), "Dark2")[i])

  lo <- pt - 1.96 * summary(lm1)$sigma
  hi <- pt + 1.96 * summary(lm1)$sigma
  for (k in 1:length(x)) {
    lines(rep(x[k] + 0.01*i, 2), c(lo[k], hi[k]), lty = 3,
          col = brewer.pal(length(x), "Dark2")[i], lwd = 2)
    points(x, imps[, "nob", i], col = brewer.pal(length(x), "Dark2")[i],
           pch = 19,
           cex = 1)
  }
}
@
}
\only<4>{
<<example1i, echo = F, fig.width = 5.25, fig.height = 3.5>>=
par(mgp = c(1, 0.6, 0), mar = c(2, 2.5,0.5, 0.5))
plot(X,
     pch = 19,
     col = grey(0.2),
     ylab = expression(X[2]),
     xlab = expression(X[1]),
     xaxt = "n",
     yaxt = "n",
     cex.lab = 1.5,
     ylim = c(range(X0$V2, -2.5, 2.4)))
points(X0$V1[is.na(X$V2)], rep(par("usr")[3], sum(is.na(X$V2))),
       xpd = T, col = 2, lwd = 1.5)
legend("bottomleft", pch = c(1,19), legend = c("missing", "observed"),
       bty = "n", col = c("red", grey(0.2)), cex = 1.3)

set.seed(2018)
add <- MASS::mvrnorm(5, c(0,0), vcov(lm1))

for (i in 1:2) {
  coef <- coef(lm1) + add[i, ]
  abline(coef, lty = 2, col = brewer.pal(length(x), "Dark2")[i])
  pt <- cbind(1, x) %*% coef
  points(x, pt, col = brewer.pal(length(x), "Dark2")[i])

  lo <- pt - 1.96 * summary(lm1)$sigma
  hi <- pt + 1.96 * summary(lm1)$sigma
  for (k in 1:length(x)) {
    lines(rep(x[k] + 0.01*i, 2), c(lo[k], hi[k]), lty = 3,
          col = brewer.pal(length(x), "Dark2")[i], lwd = 2)
    points(x, imps[, "nob", i], col = brewer.pal(length(x), "Dark2")[i],
           pch = 19,
           cex = 1)
  }
}
@
}
\only<5>{
<<example1j, echo = F, fig.width = 5.25, fig.height = 3.5>>=
par(mgp = c(1, 0.6, 0), mar = c(2, 2.5,0.5, 0.5))
plot(X,
     pch = 19,
     col = grey(0.2),
     ylab = expression(X[2]),
     xlab = expression(X[1]),
     xaxt = "n",
     yaxt = "n",
     cex.lab = 1.5,
     ylim = c(range(X0$V2, -2.5, 2.4)))
points(X0$V1[is.na(X$V2)], rep(par("usr")[3], sum(is.na(X$V2))),
       xpd = T, col = 2, lwd = 1.5)
legend("bottomleft", pch = c(1,19), legend = c("missing", "observed"),
       bty = "n", col = c("red", grey(0.2)), cex = 1.3)
set.seed(2018)
add <- MASS::mvrnorm(5, c(0,0), vcov(lm1))

for (i in 1:3) {
  coef <- coef(lm1) + add[i, ]
  abline(coef, lty = 2, col = brewer.pal(length(x), "Dark2")[i])
  pt <- cbind(1, x) %*% coef
  points(x, pt, col = brewer.pal(length(x), "Dark2")[i], lwd = 2)

  lo <- pt - 1.96 * summary(lm1)$sigma
  hi <- pt + 1.96 * summary(lm1)$sigma
  for (k in 1:length(x)) {
    lines(rep(x[k] + 0.01*i, 2), c(lo[k], hi[k]), lty = 3,
          col = brewer.pal(length(x), "Dark2")[i], lwd = 2)
    points(x, imps[, "nob", i], col = brewer.pal(length(x), "Dark2")[i],
           pch = 19,
           cex = 1)
  }
}
@
}
\only<6>{
<<example1k, echo = F, fig.width = 5.25, fig.height = 3.5>>=
par(mgp = c(1, 0.6, 0), mar = c(2, 2.5,0.5, 0.5))
plot(X,
     pch = 19,
     col = grey(0.2),
     ylab = expression(X[2]),
     xlab = expression(X[1]),
     xaxt = "n",
     yaxt = "n",
     cex.lab = 1.5,
     ylim = c(range(X0$V2, -2.5, 2.4)))
points(X0$V1[is.na(X$V2)], rep(par("usr")[3], sum(is.na(X$V2))),
       xpd = T, col = 2, lwd = 1.5)
legend("bottomleft", pch = c(1,19), legend = c("missing", "observed"),
       bty = "n", col = c("red", grey(0.2)), cex = 1.3)
set.seed(2018)
add <- MASS::mvrnorm(5, c(0,0), vcov(lm1))

for (i in 1:5) {
  coef <- coef(lm1) + add[i, ]
  abline(coef, lty = 2, col = brewer.pal(length(x), "Dark2")[i])
  pt <- cbind(1, x) %*% coef
  points(x, pt, col = brewer.pal(length(x), "Dark2")[i])

  lo <- pt - 1.96 * summary(lm1)$sigma
  hi <- pt + 1.96 * summary(lm1)$sigma
  for (k in 1:length(x)) {
    lines(rep(x[k] + 0.01*i, 2), c(lo[k], hi[k]), lty = 2,
          col = brewer.pal(length(x), "Dark2")[i], lwd = 1)
    points(x, imps[, "nob", i], col = brewer.pal(length(x), "Dark2")[i],
           pch = 19,
           cex = 1)
  }
}
@
}
\end{column}
\end{columns}
\onslide<6> In the end, we obtain one imputed dataset for each color.
\end{frame}


\subsection{Multivariate missing data}\label{subsec:multivarmissing}

\begin{frame}{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
\blue{Multivariate missing data:}\\
What if we have \blue{missing values in more than one variable}?

\vfill

\pause
\begin{columns}[onlytextwidth]
\begin{column}{0.65\textwidth}
In case of \blue{monotone missing values} we can use the technique for univariate missing data in a chain:
impute $x_4$ given $x_1$\\
impute $x_3$ given $x_1$ and $x_4$\\
impute $x_2$ given $x_1$, $x_4$ and $x_3$
\end{column}
\begin{column}{0.3\textwidth}
\scalebox{0.8}{
  \begin{tabular}{ccccc}
  $X_1$ & $X_2$ & $X_3$ & $X_4$\\\hline
  \checkmark & NA     & \checkmark & \checkmark\\
  \checkmark & NA     & NA         & \checkmark\\
  \checkmark & NA     & NA         & NA\\
  \vdots     & \vdots & \vdots     & \vdots
 \end{tabular}
}
\end{column}
\end{columns}

\vfill

\pause
\begin{columns}[onlytextwidth]
\begin{column}{0.65\textwidth}
When we have \blue{non-monotone missing data} there is no sequence without
conditioning on unobserved values.
\vfill
\end{column}
\begin{column}{0.3\textwidth}
\scalebox{0.8}{
  \begin{tabular}{ccccc}
  $X_1$ & $X_2$ & $X_3$ & $X_4$\\\hline
  \checkmark & NA         & \checkmark & \checkmark\\
  NA         & \checkmark & NA         & NA\\
  \checkmark & NA         & \checkmark & NA\\
  \vdots     & \vdots     & \vdots     & \vdots
 \end{tabular}
}
\end{column}
\end{columns}
\end{frame}



\begin{frame}[label = jointmodelimp]{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
There are \blue{two popular approaches} for the imputation step in
\blue{multivariate non-monotone} missing data:
% \begin{columns}[T, onlytextwidth]
% \begin{column}{0.4\textwidth}
\begin{block}{Fully conditional specification}
\begin{itemize}
\item Multiple Imputation using Chained Equations (\blue{MICE})
\item sometimes also: sequential regression
\item Implemented in SPSS, R, Stata, SAS, \ldots
\item our focus here
\end{itemize}
\end{block}


\pause
\begin{block}{Joint model imputation}
(more details later)
\end{block}
% \end{column}
% %
% \begin{column}{0.55\textwidth}
% \end{column}
%
% \begin{flushright}
% \includegraphics[height = 0.75\textheight]{graphics/MI-imp-cut.pdf}
% \end{flushright}
% \end{column}
% \end{columns}
\end{frame}


\begin{frame}{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
% Iterative procedure:
% \begin{itemize}
% \item draw starting values from observed data
% \item impute each variable conditional on all others
% \item repeat until convergence
% \end{itemize}

\blue{Markov Chain Monte Carlo}\\
is a technique to \blue{draw samples from a complex probability distribution} by
creating a chain of random variables (a Markov Chain). The distribution each
element in the chain is sampled from depends on the value of the previous element.
When certain conditions are met, the chain eventually stabilizes and by
continuing to sample elements of the chain a sample from the complex distribution
of interest can be obtained.

\bigskip

\pause
\blue{Gibbs sampling}\\
is an MCMC method where a \blue{sample from a multivariate distribution} is obtained
by repeatedly drawing from each of the univariate full conditional distributions
instead.
\end{frame}


\subsection{FCS/MICE}\label{subsec:micealgorithm}

\begin{frame}{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
\blue{MICE} (\blue{M}ultiple \blue{I}mputation using \blue{C}hained \blue{E}quations)
or\\
\blue{FCS} (multiple imputation using \blue{F}ully \blue{C}onditional \blue{S}pecification)

\bigskip
extends univariable imputation to the setting with multivariate non-monotone missingness:

\bigskip

MICE/FCS
\begin{itemize}
\item imputes multivariate missing data on a variable-by-variable basis,
\item using the technique for univariable missing data.
% \item requires specification of an imputation model per incomplete variable.
\end{itemize}

\bigskip

\pause
Moreover, MICE/FCS is
\begin{itemize}
\item an iterative procedure, specifically
\item a Markov Chain Monte Carlo (MCMC) method,
\item uses the idea of the Gibbs sampler, and
\item is a Gibbs sampler if the conditional distributions are compatible\\
      (we will come back to this)
\end{itemize}
\end{frame}


% \subsection{The MICE algorithm}

\begin{frame}{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
\begin{block}{Notation}
\begin{itemize}
\item $X$: $n \times p$ data matrix with $n$ rows and $p$ variables $x_1,\ldots, x_p$
\item $R$: $n \times p$ missing indicator matrix containing 0 (missing) or 1 (observed)
\end{itemize}
\end{block}

\begin{columns}[onlytextwidth]
\begin{column}{0.5\textwidth}
\begin{center}
\newcolumntype{g}{>{\columncolor{EMClight}}c}
$\bmath X$ = \begin{tabular}{|cgcc|}\arrayrulecolor{lightgray}
\multicolumn{1}{c}{$X_{-2}$} & \textcolor{EMCdark}{$X_2$} & \multicolumn{2}{c}{$X_{-2}$}\\\hline
$x_{1,1}$ & $x_{1,2}$ & \ldots & $x_{1,p}$\\
$x_{2,1}$ & $x_{2,2}$ & \ldots & $x_{2,p}$\\
\vdots    & \vdots    & $\ddots$ & \vdots\\
$x_{n,1}$ & $x_{n,2}$ & \ldots & $x_{n,p}$\\\hline
\end{tabular}\\
\end{center}
\end{column}
%
\begin{column}{0.5\textwidth}
\begin{center}
$\bmath R$ = \begin{tabular}{|cccc|}\arrayrulecolor{lightgray}
\multicolumn{4}{c}{}\\\hline
$R_{1,1}$ & $R_{1,2}$ & \ldots & $R_{1,p}$\\
$R_{2,1}$ & $R_{2,2}$ & \ldots & $R_{2,p}$\\
\vdots    & \vdots    & $\ddots$ & \vdots\\
$R_{n,1}$ & $R_{n,2}$ & \ldots & $R_{n,p}$\\\hline
\end{tabular}
\end{center}
\end{column}
\end{columns}

\bigskip

\pause
\textbf{For example:}
% $X_2$: the column of $\bmath X$ containing $x_2$ for all subjects\\
% $X_{-2}$: all columns of $\bmath X$ except $X_2$
\begin{columns}[onlytextwidth]
\begin{column}{0.5\textwidth}
\begin{center}
$\bmath X$ =
\begin{tabular}{ccccc}\arrayrulecolor{lightgray}
$X_1$ & $X_2$ & $X_3$ & $X_4$\\\hline
\checkmark & NA         & \checkmark & \checkmark\\
\checkmark & \checkmark & NA & NA\\
\checkmark & NA         & \checkmark & NA\\
\end{tabular}
\end{center}
\end{column}
\begin{column}{0.5\textwidth}
\begin{center}
\ding{225} $\bmath R$ =
\begin{tabular}{|cccc|}\arrayrulecolor{lightgray}
\hline
  % $X_1$ & $X_2$ & $X_3$ & $X_4$\\\hline
  1 & 0         & 1 & 1\\
  1 & 1 & 0 & 0\\
  1 & 0         & 1 & 0\\\hline
\end{tabular}
\end{center}
\end{column}
\end{columns}
\end{frame}



\begin{frame}[label = micealgorithm]{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}

% Algorithm
\begin{algorithm}[H]
\caption{MICE algorithm \cite{Buuren2012} for \textbf{one} imputed dataset}
\algrenewcommand\algorithmicdo{}
\begin{algorithmic}[1]
\For{$j$ in $1,\ldots, p$:}
\Comment{Setup}
\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{
       Specify imputation model for variable $X_j$\\
       $p(X_j^{mis}\mid X_j^{obs}, X_{-j}, R)$}
\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{
        Fill in starting imputations $\dot X_j^0$ by random draws
        from $X_j^{obs}$.}
\EndFor
\uncover<2->{
\Statex
\For{$t$ in $1,\ldots, T$:} \Comment{loop through iterations}
\For{$j$ in $1,\ldots, p$:} \Comment{loop through variables}
\uncover<3->{
\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{
       Define currently complete data except $X_j$\\
       $\dot X_{-j}^t = \left(\dot X_1^t,\ldots, \dot X_{j-1}^t,
               \dot X_{j+1}^{t-1},\ldots, \dot X_p^{t-1}\right)$.}
}
\uncover<4->{
\State Draw parameters $\dot \theta_j^t\sim p(\theta_j^t \mid X_j^{obs}, \dot X_{-j}^t, R)$.
}
\uncover<5->{
\State Draw imputations $\dot X_j^t \sim P(X_j^{mis}\mid \dot X_{-j}^t, R, \dot\theta_j^t)$.
}
\EndFor
\EndFor
}
\end{algorithmic}
% % \caption{pseudocode for the calculation of }
% % \label{alg:seq}
\end{algorithm}
\end{frame}

\begin{frame}{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}

% Algorithm
\setcounter{algorithm}{0}
\begin{algorithm}[H]
\caption{MICE algorithm \cite{Buuren2012} for \textbf{one} imputed dataset}
\algrenewcommand\algorithmicdo{}
\begin{algorithmic}[1]
\For{$j$ in $1,\ldots, p$:}
\Comment{Setup}
\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{
       Specify imputation model for variable $X_j$\\
       $p(X_j^{mis}\mid X_j^{obs}, X_{-j}, R)$}
\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{
        Fill in starting imputations $\dot X_j^0$ by random draws
        from $X_j^{obs}$.}
\EndFor
\Statex
\only<1>{%
\For{\textcolor{darkred}{$t = 1$}:}\Comment{loop through \textcolor{darkred}{iterations}}
\For{\textcolor{blue}{$j = 1$}:}\Comment{loop through \textcolor{blue}{variables}}
\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{
       Define currently complete data except $X_{\textcolor{blue}{1}}$\\
       $\dot X_{\textcolor{blue}{-1}}^{\textcolor{darkred}{1}} =
       \left(\dot X_2^{\textcolor{darkred}{0}},
             \dot X_3^{\textcolor{darkred}{0}},
             \dot X_4^{\textcolor{darkred}{0}}\right)$.}
\State Draw parameters $\dot \theta_{\textcolor{blue}{1}}^{\textcolor{darkred}{1}}\sim
p(\theta_{\textcolor{blue}{1}}^{\textcolor{darkred}{1}} \mid X_{\textcolor{blue}{1}}^{obs},
\dot X_{\textcolor{blue}{-1}}^{\textcolor{darkred}{1}}, R)$.
\State Draw imputations $\dot X_{\textcolor{blue}{1}}^{\textcolor{darkred}{1}} \sim
P(X_{\textcolor{blue}{1}}^{mis}\mid \dot X_{\textcolor{blue}{-1}}^{\textcolor{darkred}{1}}, R,
\dot\theta_{\textcolor{blue}{1}}^{\textcolor{darkred}{1}})$.
}
\only<2>{%
\For{\textcolor{darkred}{$t = 1$}:}\Comment{loop through \textcolor{darkred}{iterations}}
\For{\textcolor{blue}{$j = 2$}:}\Comment{loop through \textcolor{blue}{variables}}
\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{
       Define currently complete data except $X_{\textcolor{blue}{2}}$\\
       $\dot X_{\textcolor{blue}{-2}}^{\textcolor{darkred}{1}} =
       \left(\dot X_1^{\textcolor{darkred}{1}},
             \dot X_3^{\textcolor{darkred}{0}},
             \dot X_4^{\textcolor{darkred}{0}}\right)$.}
\State Draw parameters $\dot \theta_{\textcolor{blue}{2}}^{\textcolor{darkred}{1}}\sim
p(\theta_{\textcolor{blue}{2}}^{\textcolor{darkred}{1}} \mid X_{\textcolor{blue}{2}}^{obs},
\dot X_{\textcolor{blue}{-2}}^{\textcolor{darkred}{1}}, R)$.
\State Draw imputations $\dot X_{\textcolor{blue}{2}}^{\textcolor{darkred}{1}} \sim
P(X_{\textcolor{blue}{2}}^{mis}\mid \dot X_{\textcolor{blue}{-2}}^{\textcolor{darkred}{1}}, R,
\dot\theta_{\textcolor{blue}{2}}^{\textcolor{darkred}{1}})$.
}
\only<3>{%
\For{\textcolor{darkred}{$t = 1$}:}\Comment{loop through \textcolor{darkred}{iterations}}
\For{\textcolor{blue}{$j = 3$}:}\Comment{loop through \textcolor{blue}{variables}}
\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{
       Define currently complete data except $X_{\textcolor{blue}{3}}$\\
       $\dot X_{\textcolor{blue}{-3}}^{\textcolor{darkred}{1}} =
       \left(\dot X_1^{\textcolor{darkred}{1}},
             \dot X_2^{\textcolor{darkred}{1}},
             \dot X_4^{\textcolor{darkred}{0}}\right)$.}
\State Draw parameters $\dot \theta_{\textcolor{blue}{3}}^{\textcolor{darkred}{1}}\sim
p(\theta_{\textcolor{blue}{3}}^{\textcolor{darkred}{1}} \mid X_{\textcolor{blue}{3}}^{obs},
\dot X_{\textcolor{blue}{-3}}^{\textcolor{darkred}{1}}, R)$.
\State Draw imputations $\dot X_{\textcolor{blue}{3}}^{\textcolor{darkred}{1}} \sim
P(X_{\textcolor{blue}{3}}^{mis}\mid \dot X_{\textcolor{blue}{-3}}^{\textcolor{darkred}{1}}, R,
\dot\theta_{\textcolor{blue}{3}}^{\textcolor{darkred}{1}})$.
}
\only<4>{%
\For{\textcolor{darkred}{$t = 1$}:}\Comment{loop through \textcolor{darkred}{iterations}}
\For{\textcolor{blue}{$j = 4$}:}\Comment{loop through \textcolor{blue}{variables}}
\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{
       Define currently complete data except $X_{\textcolor{blue}{4}}$\\
       $\dot X_{\textcolor{blue}{-4}}^{\textcolor{darkred}{1}} =
       \left(\dot X_1^{\textcolor{darkred}{1}},
             \dot X_2^{\textcolor{darkred}{1}},
             \dot X_3^{\textcolor{darkred}{1}}\right)$.}
\State Draw parameters $\dot \theta_{\textcolor{blue}{4}}^{\textcolor{darkred}{1}}\sim
p(\theta_{\textcolor{blue}{4}}^{\textcolor{darkred}{1}} \mid X_{\textcolor{blue}{4}}^{obs},
\dot X_{\textcolor{blue}{-4}}^{\textcolor{darkred}{1}}, R)$.
\State Draw imputations $\dot X_{\textcolor{blue}{4}}^{\textcolor{darkred}{1}} \sim
P(X_{\textcolor{blue}{4}}^{mis}\mid \dot X_{\textcolor{blue}{-4}}^{\textcolor{darkred}{1}}, R,
\dot\theta_{\textcolor{blue}{4}}^{\textcolor{darkred}{1}})$.
}
\only<5>{%
\For{\textcolor{darkred}{$t = 2$}:}\Comment{loop through \textcolor{darkred}{iterations}}
\For{\textcolor{blue}{$j = 1$}:}\Comment{loop through \textcolor{blue}{variables}}
\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{
       Define currently complete data except $X_{\textcolor{blue}{1}}$\\
       $\dot X_{\textcolor{blue}{-1}}^{\textcolor{darkred}{2}} =
       \left(\dot X_2^{\textcolor{darkred}{1}},
             \dot X_3^{\textcolor{darkred}{1}},
             \dot X_4^{\textcolor{darkred}{1}}\right)$.}
\State Draw parameters $\dot \theta_{\textcolor{blue}{1}}^{\textcolor{darkred}{2}}\sim
p(\theta_{\textcolor{blue}{1}}^{\textcolor{darkred}{2}} \mid X_{\textcolor{blue}{1}}^{obs},
\dot X_{\textcolor{blue}{-1}}^{\textcolor{darkred}{2}}, R)$.
\State Draw imputations $\dot X_{\textcolor{blue}{1}}^{\textcolor{darkred}{2}} \sim
P(X_{\textcolor{blue}{1}}^{mis}\mid \dot X_{\textcolor{blue}{-1}}^{\textcolor{darkred}{2}}, R,
\dot\theta_{\textcolor{blue}{1}}^{\textcolor{darkred}{2}})$.
}
\only<6>{%
\For{\textcolor{darkred}{$t = 2$}:}\Comment{loop through \textcolor{darkred}{iterations}}
\For{\textcolor{blue}{$j = 2$}:}\Comment{loop through \textcolor{blue}{variables}}
\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{
       Define currently complete data except $X_{\textcolor{blue}{2}}$\\
       $\dot X_{\textcolor{blue}{-2}}^{\textcolor{darkred}{2}} =
       \left(\dot X_1^{\textcolor{darkred}{2}},
             \dot X_3^{\textcolor{darkred}{1}},
             \dot X_4^{\textcolor{darkred}{1}}\right)$.}
\State Draw parameters $\dot \theta_{\textcolor{blue}{2}}^{\textcolor{darkred}{2}}\sim
p(\theta_{\textcolor{blue}{2}}^{\textcolor{darkred}{2}} \mid X_{\textcolor{blue}{2}}^{obs},
\dot X_{\textcolor{blue}{-2}}^{\textcolor{darkred}{2}}, R)$.
\State Draw imputations $\dot X_{\textcolor{blue}{2}}^{\textcolor{darkred}{2}} \sim
P(X_{\textcolor{blue}{2}}^{mis}\mid \dot X_{\textcolor{blue}{-2}}^{\textcolor{darkred}{2}}, R,
\dot\theta_{\textcolor{blue}{2}}^{\textcolor{darkred}{2}})$.
}
\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}
\end{frame}


\begin{frame}{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}


The imputed values from the \blue{last iteration},
$$\left(\dot X_1^T, \ldots, \dot X_p^T\right),$$
are then used to replace the missing values in the original data.


\bigskip

One run through the algorithm \blue{\ding{225}} one imputed dataset.

\bigskip

\pause
\blue{\ding{225}} To obtain $m$ imputed datasets: \blue{repeat $m$ times}

\bigskip

\pause
We refer to the \blue{sequence of imputations} for one missing value, from
starting value to final iteration, as a \blue{chain}.
Each run through the MICE algorithm produces one chain per missing value.
\end{frame}



\begin{frame}{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
\blue{Why iterations?}
\begin{itemize}
\item Imputed values in one variable depend on the imputed values of the other variables
(Gibbs sampling).
\item If the starting values (random draws) are far from the actual distribution,
imputed values from the first few iterations are not draws from the distribution
of interest.
\end{itemize}

\bigskip

\pause
\blue{How many iterations?}\\
Until \blue{convergence}\\
= when the sampling distribution does not change any more\\
(Note: the imputed value will still vary between iterations.)

\bigskip

\blue{How to evaluate convergence?}\\
The \blue{traceplot} (x-axis: iteration number, y-axis: imputed value) should
show a horizontal band
\end{frame}

\subsection{Checking convergence}\label{subsec:convergence}
<<sim_convergence, fig.width = 6, fig.height = 4>>=
library(ggplot2)
library(reshape2)

d <- 100
m <- 3
init <- c(-50, 50, 20)
x <- sapply(init, rnorm, n = d, sd = 0.1)
colnames(x) <- 1:ncol(x)
xorig <- rnorm(d/5, 3, 1)
mus <- sds <- matrix(nrow = 0, ncol = m)
for (i in 1:2000) {
  set <- x[nrow(x):(nrow(x) - d + 1), ]
  samp <- rbind(cbind(xorig, xorig, xorig), set)
  mu <- colMeans(samp) + rgamma(3, 0.1, 0.1)
  sd <- pmin(3, apply(samp, 2, sd))
  mus <- rbind(mus, mu)
  sds <- rbind(sds, sd)
  x <- rbind(x, rnorm(m, mu, sd))
}



ggplot(melt(x[d:nrow(x), ]), aes(x = Var1, y = value,
                                 color = factor(Var2))) +
  geom_line() +
  scale_color_brewer(palette = "Dark2", name = "", labels = paste("chain", 1:3)) +
  theme_light() +
  theme(legend.position = c(0.9, 0.15)) +
  xlab("iteration")
@

\begin{frame}[label = convergence]{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
\begin{center}
\includegraphics[width = 0.7\linewidth]{figure/sim_convergence-1.pdf}
\end{center}


Each chain is the sequence of imputed values (from starting value to final imputed value)
for the same missing value.
\end{frame}


\begin{frame}[label = convergence2]{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
In imputation we have
\begin{itemize}
\item several \blue{variables} with missing values (e.g., $p$)
\item several missing \blue{values} in each of these variables
\item $m$ \blue{chains} for each missing value
\end{itemize}
\blue{\ding{225}} possibly a large number of MCMC chain

\bigskip

To check all chains separately could be very time consuming in large datasets
(and storing all iterations from all imputed values is inefficient).

\bigskip

\pause
\blue{Alternative:} Calculate and plot a summary (e.g., the mean) of the imputed
values over all subjects, separately per chain and variable\\
\blue{\ding{225}} only $m \times p$ chains to check
\end{frame}


<<sim_conv_summary, echo = F>>=
N = 5
p = 4
m = 3
maxit <- 20

dat <- expand.grid(list(id = paste("ID", 1:N),
                        x = paste0("x", 1:p),
                        imp = paste0("imp", 1:m),
                        it = 1:maxit))

coef <- c(0, 0.5, 0.2, 0.4, -0.1, -1, 0.5, 1)
X <- model.matrix(~ id + x, data = dat)
mu <- X %*% coef
dat$y <- rnorm(nrow(dat), mean = mu, sd = 0.5)

library(plyr)
meandat <- ddply(dat, c("x", "imp", "it"), summarize, mean = mean(y))
@

\begin{frame}[label = convergence_multiple]{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
\only<1>{
<<convplot1a, echo = F, fig.width = 7, fig.height = 4.5>>=
library(ggplot2)
ggplot(dat, aes(x = it, y = y, color = imp)) +
  geom_line() +
  facet_grid(id ~ x, scales = 'free') +
  theme(legend.position = 'bottom') +
  xlab("iteration") +
  ylab("imputed value") +
  scale_color_brewer(palette = "Dark2",
                     name = "imputation number:")
@
}
\only<2>{
<<convplot1b, echo = F, fig.width = 7.5, fig.height = 5>>=
g2 <- ggplot(dat, aes(x = it, y = y, color = imp, group = id)) +
  facet_grid(imp ~ x, scales = 'free',
             labeller = labeller(imp = c("imp1" = "imputation 1",
                                         "imp2" = "imputation 2",
                                         "imp3" = "imputation 3"))) +
  theme(legend.position = 'bottom') +
  xlab("iteration") +
  ylab("imputed value") +
  scale_color_brewer(palette = "Dark2",
                     name = "imputation number:",
                     labels = c(1:3))

g2 + geom_line()

@
}
\only<3>{
<<convplot1c, echo = F, fig.width = 7.5, fig.height = 5>>=
g2 + geom_line(alpha = 0.5) +
  geom_line(data = meandat, aes(x = it, y = mean, group = 1), lwd = 1)
@
}
\only<4>{
<<convplot1d, echo = F, fig.width = 7.5, fig.height = 2.5>>=
ggplot(meandat, aes(x = it, y = mean, color = imp)) +
  geom_line(lwd = 0.7) +
  facet_wrap("x", scales = 'free', ncol = 4) +
  theme(legend.position = 'bottom') +
  xlab("iteration") +
  ylab("imputed value") +
  scale_color_brewer(palette = "Dark2",
                     name = "imputation number:",
                     labels = c(1:3))
@
}
\end{frame}


<<MIexample_analysis, include = F>>=
library(mice)
set.seed(123)
N <- 100
p <- 4
m <- 3

S <- diag(c(0.5, 5, 1, 1))
s <- c(0.7, -0.5, 0.15, 0.15, 0.1, 0.2)
S[upper.tri(S)] <- s
S[lower.tri(S)] <- s

X <- MASS::mvrnorm(n = N,
             mu = c(0.2, 12, 3, 0.2),
             Sigma = S)
colnames(X) <- paste0("x", 1:4)

DF_orig <- as.data.frame(cbind(id = 1:N, X))
DF_orig$id <- as.integer(DF_orig$id)

p1 <- rbinom(n = nrow(DF_orig), size = 1, prob = plogis(-5 + 2*DF_orig$x3))
p2 <- rbinom(n = nrow(DF_orig), size = 1, prob = plogis(4 - DF_orig$x3))
p3 <- rbinom(n = nrow(DF_orig), size = 1, prob = plogis(4 - DF_orig$x3))

p1[1:3] <- c(0, 1, 0)
p2[1:3] <- c(1, 0, 1)
p3[1:3] <- c(1, 0, 0)

DF_orig[p1 == 0, "x2"] <- NA
DF_orig[p2 == 0, "x3"] <- NA
DF_orig[p3 == 0, "x4"] <- NA

imp <- mice(DF_orig, m = m)
# round(complete(imp, 1)[1:3, ], 1)
res <- with(imp, lm(x1 ~ x2 + x3 + x4))

for (i in 1:3) {
  subres <- round(summary(res$analyses[[i]])$coef[,1:2], 2)
  pr <- paste0(paste0("$\\beta_", 0:3, "$"), " & ",
               apply(subres, 1, paste0, collapse = " & "), collapse = "\\\\\n")

  assign(paste0("print", i), pr)
}
@

\section{Analysis step}
\begin{frame}{\thesection. \insertsection}
Multiple imputed datasets:
\begin{columns}
\begin{column}{0.3\linewidth}
\scalebox{0.8}{
  \begin{tabular}{ccccc}
  $X_1$ & $X_2$ & $X_3$ & $X_4$\\\hline
   1.4 & \cellcolor{EMClight} 9.2  & 1.8                      & 2.0\\
   0.5 & 12.4                      & \cellcolor{EMClight} 2.3 & \cellcolor{EMClight} 0.1\\
  -0.5 & \cellcolor{EMClight} 10.7 & 2.6                      & \cellcolor{EMClight} -1.6\\
  \vdots    & \vdots     & \vdots     & \vdots
 \end{tabular}
}
\end{column}
\begin{column}{0.3\textwidth}
\scalebox{0.8}{
  \begin{tabular}{ccccc}
  $X_1$ & $X_2$ & $X_3$ & $X_4$\\\hline
   1.4 & \cellcolor{EMClight} 13.3 & 1.8                      & 2.0\\
   0.5 & 12.4                      & \cellcolor{EMClight} 2.1 & \cellcolor{EMClight} 0.6\\
  -0.5 & \cellcolor{EMClight} 10.2 & 2.6                      & \cellcolor{EMClight} -1.7\\
  \vdots    & \vdots     & \vdots     & \vdots
 \end{tabular}
}
\end{column}
\begin{column}{0.3\textwidth}
\scalebox{0.8}{
  \begin{tabular}{ccccc}
  $X_1$ & $X_2$ & $X_3$ & $X_4$\\\hline
   1.4 & \cellcolor{EMClight} 10.0 & 1.8                      & 2.0\\
   0.5 & 12.4                      & \cellcolor{EMClight} 2.2 & \cellcolor{EMClight} -1.4\\
  -0.5 & \cellcolor{EMClight} 8.6  & 2.6                      & \cellcolor{EMClight} -1.0\\
  \vdots    & \vdots     & \vdots     & \vdots
 \end{tabular}
}
\end{column}
\end{columns}

\vspace{2ex}
\pause
Analysis model of interest, e.g.,
\begin{beamercolorbox}{block body}
    $$x_1 = \beta_0 + \beta_1 x_2 + \beta_2 x_3 + \beta_3 x_4$$
\end{beamercolorbox}
\vspace*{2ex}

\pause
Multiple sets of results:
\begin{columns}
\begin{column}{0.3\textwidth}
\begin{tabular}{l|rr}
& est. & se\\\hline
\Sexpr{print1}
\end{tabular}
\end{column}
\begin{column}{0.3\textwidth}
\begin{tabular}{l|rr}
& est. & se\\\hline
\Sexpr{print2}
\end{tabular}
\end{column}
\begin{column}{0.3\textwidth}
\begin{tabular}{l|rr}
& est. & se\\\hline
\Sexpr{print3}
\end{tabular}
\end{column}
\end{columns}
\end{frame}

\section{Pooling}
\subsection{Why pooling?}
\begin{frame}{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
Recall from slide~\ref{RubinsIdeas}:\\
We need to represent missing values by a \blue{number of imputations}.\\
\blue{\ding{225}} $m$ imputed datasets

\bigskip

\pause

From the different imputed datasets we get \blue{different sets of parameter estimates},
each of them with a standard error, representing the uncertainty about the
estimate.

\bigskip

\pause
We want to \blue{summarize} the results and describe
\blue{how (much) the results vary} between the imputed datasets.

\end{frame}


\begin{frame}[fragile]{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
In the results from multiply imputed data there are
\blue{two types of variation/uncertainty}:
\begin{itemize}
\item within imputation (represented by the confidence intervals)
\item between imputation (horizontal shift between imputations)
\end{itemize}

<<poolplot1a, echo = F, fig.width = 6, fig.height = 1.5>>=
reslist <- lapply(seq_along(res$analyses),
                  function(i) {
                    x <- res$analyses[[i]]
                    a <- as.data.frame(cbind(coef(x), confint(x)))
                    names(a) <- c("coef", "lo", "hi")
                    a$imp <- i
                    a$var <- rownames(a)
                    a
                    })

poolDF <- do.call(rbind, reslist)

ppool0 <- ggplot(poolDF, aes(x = coef, y = imp)) +
  geom_point() +
  geom_errorbarh(aes(xmin = lo, xmax = hi), height = 0.1) +
  facet_grid(~var, scales = 'free') +
  theme_light() +
  theme(strip.text = element_text(size = 12, face = "bold")) +
  scale_y_discrete(limits = c(1, 2, 3),
                   labels = c(paste("imp", 1:3))) +
  # scale_y_discrete(limits = c("pooled", 1,2,3),
  #                  labels = c("pooled", paste("imp", 1:3))) +
  ylab("") +
  scale_x_continuous(breaks = NULL) +
  xlab("parameter estimate & 95% confidence interval")
ppool0
@
\end{frame}

<<poolcalc, echo = F>>=
library(plyr)

seDF <- do.call(rbind,
        lapply(seq_along(res$analyses), function(i){
          ses <- as.data.frame(summary(res$analyses[[i]])$coef[, 2, drop = F])
          ses$var <- rownames(ses)
          ses$imp <- i
          ses
        })
)
means <- ddply(poolDF, "var", summarize, coefmean = mean(coef))
ses <- ddply(seDF, "var", summarize, semean = mean(`Std. Error`))

CIs <- ddply(poolDF, "var", summarize, himean = mean(hi), lomean = mean(lo))

pooled <- as.data.frame(summary(pool(res, method = "standard"))[, c("est", "lo 95", "hi 95")])
pooled$imp <- "pooled"
pooled$var <- rownames(reslist[[1]])
@


\begin{frame}[fragile]{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
To summarize the results, we can take the mean of the results from the separate
analyses. This is the \blue{pooled point estimate}.

\only<1>{
<<poolplot1b, echo = F, fig.width = 6, fig.height = 1.5>>=
ppool0 +
  geom_vline(data = ddply(poolDF, "var", summarize, coefmean = mean(coef)),
             aes(xintercept = coefmean), lty = 2)
@
}
\only<2->{
<<poolplot1c, echo = F, fig.width = 6, fig.height = 1.5>>=
ppool0 +
  geom_vline(data = ddply(poolDF, "var", summarize, coefmean = mean(coef)),
             aes(xintercept = coefmean), lty = 2) +
  geom_vline(data = CIs, aes(xintercept = himean), col = 2, lty = 2) +
  geom_vline(data = CIs, aes(xintercept = lomean), col = 2, lty = 2)
@
}
\onslide<2->{%
But does the same work for the std. error (or bounds of the CIs)?
}

\bigskip

\onslide<3->{%
The averaged CI's (marked in red) seem to underestimate the total variation
(within + between).
}
\end{frame}


\subsection{Rubin's Rules}
\begin{frame}{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
The most commonly used method to pool results from analyses of multiply imputed
data was introduced by Rubin \cite{Rubin1987}, hence \blue{Rubin's Rules}.

\bigskip

\blue{Notation:}\\
$m$: number of imputed datasets\\
$Q_\ell$: quantity of interest (e.g., regr. parameter $\beta$) from $\ell$-th imputation\\
$U_\ell$: variance of $Q_\ell$ (e.g., $var(\beta) = se(\beta)^2$)

\bigskip

\blue{Pooled parameter estimate:}
$$\bar Q = \frac{1}{m} \sum_{\ell = 1}^m\hat Q_\ell$$
\end{frame}


\begin{frame}{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
The \blue{variance} of the pooled parameter estimate is calculated from the
\blue{within and between imputation variance}.


\bigskip

\blue{Average within imputation variance:}
$$\bar U = \frac{1}{m} \sum_{\ell = 1}^m \bar U_\ell$$

\bigskip
\blue{Between imputation variance:}
$$B = \frac{1}{m-1}\sum_{\ell = 1}^m \left(\hat Q_\ell - \bar Q\right)^T\left(\hat Q_\ell - \bar Q\right)$$

\bigskip

\blue{Total variance:}
$$T = \bar U + B + B/m$$

\end{frame}


<<poolplot1d, echo = F, fig.width = 6, fig.height = 1.5, eval = F>>=
ppool0 +
  geom_vline(data = ddply(poolDF, "var", summarize, coefmean = mean(coef)),
             aes(xintercept = coefmean), lty = 2) +
  geom_vline(data = CIs, aes(xintercept = himean), col = 2, lty = 2, alpha = 0.5) +
  geom_vline(data = CIs, aes(xintercept = lomean), col = 2, lty = 2, alpha = 0.5) +
  geom_vline(data = pooled, aes(xintercept = `lo 95`), lty = 2) +
  geom_vline(data = pooled, aes(xintercept = `hi 95`), lty = 2)
@

<<calc-by-hand, echo = F, eval = F>>=
m <- 3
alpha = 0.05
Q <- means[,2]
B <- 1/(m - 1) * colSums(matrix(nrow = m, data = (poolDF[, 1] - rep(Q, m))^2, byrow = T))
U <- ses[, 2]^2
T <- U + B + B/m

r <- (B + B/m)/U

nu = (m - 1) * (1 + mic$r^{-1})^2

gamma = (r + 2/(nu+3))/(r+1)

Q + qt(alpha/2, df = nu) * T^{1/2}
Q - qt(alpha/2, df = nu) * T^{1/2}

1 - pf(Q^2/T, df1 = 1, df2 = nu)

@

\begin{frame}[label=poolingdf]{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
\blue{Confidence intervals} for pooled estimates can be obtained using the
\blue{pooled standard error} $\sqrt{T}$ and a \blue{reference $t$ distribution}
with degrees of freedom

$$\nu = \left(m - 1\right)\left(1 + r_m^{-1}\right)^2,$$
where $r_m  = \frac{\left(B + B/m \right)}{\bar U}$
is the relative increase in variance that is due to the missing values.

\bigskip

The \blue{$(1 - \alpha)$ 100\%  confidence interval} is then
$$\bar Q \pm t_\nu(\alpha/2)\sqrt{T},$$
where $t_{\nu}$ is the $\alpha/2$ quantile of the $t$ distribution with $\nu$
degrees of freedom.
\end{frame}

\begin{frame}[fragile]{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
<<cis_final, echo = F, fig.width = 6, fig.height = 1.5>>=
CIDF <- data.frame(
  x = rep(unlist(t(pooled[, 2:3])), each = 2),
  y = rep(c(0.5, 3.5)[c(1,2,2,1)], nrow(pooled)),
  var = rep(rownames(pooled), each = 4)
)

ggplot(poolDF, aes(x = coef, y = as.numeric(imp))) +
  geom_polygon(data = CIDF, aes(x = x, y = y), fill = "blue4", alpha = 0.2) +
  geom_point() +
  geom_errorbarh(aes(xmin = lo, xmax = hi), height = 0.1) +
  facet_grid(~var, scales = 'free') +
  theme_light() +
  theme(strip.text = element_text(size = 12, face = "bold")) +
  scale_y_discrete(limits = c(1, 2, 3),
                   labels = c(paste("imp", 1:3))) +
  ylab("") +
  scale_x_continuous(breaks = NULL) +
  xlab("parameter estimate & 95% confidence interval") +
  geom_vline(data = ddply(poolDF, "var", summarize, coefmean = mean(coef)),
             aes(xintercept = coefmean), lty = 2)
  # geom_vline(data = CIs, aes(xintercept = himean), col = 2, lty = 2, alpha = 0.5) +
  # geom_vline(data = CIs, aes(xintercept = lomean), col = 2, lty = 2, alpha = 0.5)
  # geom_vline(data = pooled, aes(xintercept = `lo 95`), lty = 2) +
  # geom_vline(data = pooled, aes(xintercept = `hi 95`), lty = 2)

@

\vfill

\pause

The corresponding \blue{p-value} is the probability
$$Pr\left\{F_{1,\nu} > \left(Q_0-\bar Q_m\right)^2/T\right\},$$

where $F_{1,\nu}$ is a random variable that has an F distribution with
$1$ and $\nu$ degrees of freedom, and $Q_0$ is the null hypothesis value
(typically zero).
\end{frame}


\begin{frame}
\begin{beamercolorbox}[sep=8pt,center,wd=\textwidth]{part title}
\usebeamerfont{part title}
Quiz
\end{beamercolorbox}

\vfill

To reiterate the content of the above sections, you can take the corresponding
quiz. An interactive version can be found at
\begin{block}{}
\centering\url{https://emcbiostatistics.shinyapps.io/MICourse_Quiz_PartI}
\end{block}
or you can download an html version from Canvas (Practicals/Quiz\_PartI\_static.html).
\textcolor{red}{[update links]}.

\vfill
\end{frame}
