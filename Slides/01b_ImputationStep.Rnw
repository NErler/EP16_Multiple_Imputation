\section{A closer look at the imputation step}
\subsection{Bayesian multiple imputation}

\begin{frame}{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
The imputation step consists itself of two (or three) steps:
\begin{enumerate} \setcounter{enumi}{-1}
\item Specification of the imputation model,
\item \blue{estimation} or sampling \blue{of the parameters}, and
\item \blue{drawing imputed values} from the predictive distribution.
\end{enumerate}

\bigskip

\pause
\textbf{Notation:}\\
Let $\bmath y$ be the incomplete covariate to be imputed,
and $\bmath X$ the design matrix of other (complete or imputed) variables.

$$
\bmath y = \begin{array}{c}
\bmath y_{obs}\scalebox{1.4}{\Bigg\{}\\[3ex]
\bmath y_{mis}\scalebox{1.4}{\Bigg\{}
\end{array}
\left[
\begin{array}{c}
            y_1\\
            \vdots\\
            y_q\\
            NA\\
            \vdots\\
            NA
            \end{array}
\right]
\qquad
\bmath X = \begin{array}{c}
\bmath X_{obs}\scalebox{1.4}{\Bigg\{}\\[3ex]
\bmath X_{mis}\scalebox{1.4}{\Bigg\{}
\end{array}
\left[
\begin{array}{cccc}
1 & x_{11} & \ldots & x_{1p}\\
\vdots & \vdots & \ldots & \vdots\\
1 & x_{q1} & \ldots & x_{qp}\\
1 & x_{q+1,1} & \ldots & x_{q+1,p}\\
\vdots & \vdots & \ldots & \vdots\\
1 & x_{n1} & \ldots & x_{np}\\
\end{array}
\right]
$$

\end{frame}


\begin{frame}{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}

In \blue{Bayesian imputation} (the approach seen on slides
\ref{BayesianImputationI}/\ref{BayesianImputationII} and \ref{micealgorithm})
the parameters (regression coefficients
$\bmath\beta$ and residual variance $\sigma$) are sampled from their respective
posterior distributions (step 1).

\bigskip

Imputed values are drawn from $p(\bmath y_{mis} \mid \bmath X_{mis}, \bmath{\hat\beta}, \hat\sigma)$.

\bigskip

For a normal imputation model $p()$ is the normal distribution and
$$\bmath y_{mis} = \bmath X_{mis} \hat{\bmath\beta} + \bmath{\hat{\varepsilon}}$$
where
$\bmath{\hat\varepsilon} = (\hat\varepsilon_{n}, \ldots, \hat\varepsilon_{q+1})^T$
is drawn independently from $N(0, \hat\sigma)$ (step 2).
\end{frame}


\subsection{Bootstrap multiple imputation}

\begin{frame}{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
An alternative approach is to capture the uncertainty with \blue{bootstrap} sampling.

\bigskip

In empirical \blue{Bootstrap}, (many) replications of the data are created by
repeatedly drawing values from the original data.

\pause
\tcbset{width=(\linewidth-2mm)/3,before=,after=\hfill,
colframe=blue!75!black,colback=white, size = title, valign = center,
halign = center, size = title, height = 2.5cm, width = 2cm}

\vfill

\begin{columns}
\begin{column}{0.55\linewidth}
\scalebox{0.68}{
\setlength{\unitlength}{1cm}
\begin{picture}(10, 9)
%
\put(0, 4.5){
\begin{tcolorbox}
  observed data
\end{tcolorbox}}
\put(2.2, 6.5){\vector(3,4){1}}
\put(2.2, 5.5){\vector(1,-1){1}}
\put(2.2, 4.5){\vector(1,-2){1}}
%
\put(3.2, 6.5){
\begin{tcolorbox}
  bootstrap sample
\end{tcolorbox}
}
\put(4.2, 5.8){\vdots
}
\put(3.2, 3){
\begin{tcolorbox}
  bootstrap sample
\end{tcolorbox}
}
\put(3.2, 0){
\begin{tcolorbox}
  bootstrap sample
\end{tcolorbox}
}
\put(5.4, 7.2){\vector(1,0){1}}
\put(5.4, 4.2){\vector(1,0){1}}
\put(5.4, 1.2){\vector(1,0){1}}
%
\put(6.7, 7.1){\large estimate $\boldsymbol{\hat\beta}$ and $\hat\sigma$}
\put(6.7, 4.1){estimate $\boldsymbol{\hat\beta}$ and $\hat\sigma$}
\put(6.7, 1.1){estimate $\boldsymbol{\hat\beta}$ and $\hat\sigma$}
\end{picture}
}
\end{column}
\begin{column}{0.45\linewidth}
Bootstrap samples can contain some \blue{observations multiple times} and some
\blue{observations not at all}.

\bigskip

The statistic of interest is then calculated on each of the bootstrap samples.
\end{column}
\end{columns}
\end{frame}


\begin{frame}{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
In \blue{bootstrap multiple imputation},
\begin{itemize}
\item \blue{one bootstrap sample} of the \blue{observed data} is created per imputation,
\item the \blue{least-squares estimates} of the parameters are calculated from
      $$\bmath y_{obs} = \bmath X_{obs}
      \underset{\stackrel{\downarrow}{\bmath{\hat\beta}}}{\bmath\beta} +
      \underset{\stackrel{\downarrow}{\hat\sigma}}{\varepsilon_{obs}}\hspace{2cm}\text{(step 1)}.$$
\item Imputed values are sampled from
$p(\bmath y_{mis} \mid \bmath X_{mis}, \bmath{\hat\beta}, \hat\sigma)$ (step 2).
\end{itemize}

\pause
Analogous to Bayesian multiple imputation, for a normal imputation model,
$p()$ is the normal distribution and
$$\bmath y_{mis} = \bmath X_{mis} \hat\beta + \tilde\varepsilon$$
where $\bmath{\hat\varepsilon}$ is drawn independently from $N(0, \sigma^2)$.
\end{frame}


\subsection{Semi-parametric imputation}
\begin{frame}{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
Both Bayesian and bootstrap multiple imputation sample imputed values from
a distribution $p()$ in step 2.

\bigskip

Sometimes, the empirical distribution can not
be adequately approximated by a known probability distribution.

\vfill

<<echo = F, fig.width = 8, fig.height = 3>>=
N <- 100
x1 <- runif(N, min = -1.5, max = 2)
x2 <- rnorm(N, -2, 0.5)
x3 <- rnorm(N, 3, 1)
x4 <- rbinom(N/2, size = 6, prob = 0.4)
x <- c(x1, x2, x3, x4, x4 - 4.23) + 4.5
par(mar = c(2, 3, 0.5, 0.5), mgp = c(2,0.6,0))
hist(x, nclass = 50, xlab = "", main = "")
@
\end{frame}


\begin{frame}{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
\blue{Predictive Mean Matching (PMM)} was developed to provide a semi-parametric
approach to imputation for settings where the normal distribution is not a
good choice for the predictive distribution.\cite{Little1988, Rubin1986}

\bigskip

The idea is to \blue{find cases in the observed data that are similar to the
cases with missing values} and to fill in the missing value with the observed
value from one of those cases.

\bigskip

To find similar cases, the predicted values of complete and incomplete cases
are compared.

\end{frame}



\begin{frame}[label=pmmalgo]{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
\blue{The steps in PMM:}
\begin{enumerate}
\item Obtain parameter estimates for $\bmath{\hat\beta}$ and $\hat\sigma$ (see later)
\item Calculate the predicted values for the observed data
      $$\bmath{\hat y}_{obs} = \bmath{X}_{obs} \bmath{\hat\beta}$$
\item Calculate the predicted value for the incomplete data
      $$\bmath{\hat y}_{mis} = \bmath{X}_{mis} \bmath{\hat\beta}$$
\item For each missing value, find $d$ donor candidates that fulfill a given
      criterium (details on the next slide).
\item Randomly select one of the donors.
\end{enumerate}
\end{frame}

\begin{frame}{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
Several \blue{criteria to select donors} have been proposed:
\begin{enumerate}\itemsep2mm
\item The donor is the \blue{(one) case with the smallest absolute difference}
      $\left|\hat y_{mis,i} - \hat y_{obs, j}\right|, \; j = 1,\ldots,q$.
\item Donor candidates are the \blue{$d$ cases with the smallest absolute difference}
      $\left|\hat y_{mis,i} - \hat y_{obs, j}\right|, \; j = 1,\ldots,q$.
      The donor is selected randomly from the candidates.
\item Donor candidates are those cases for which the \blue{absolute difference
      is smaller than some limit $\eta$}:
      $\left|\hat y_{mis,i} - \hat y_{obs, j}\right|<\eta, \; j = 1,\ldots,q$.
      The donor is selected randomly from the candidates.
\item Select candidates like in 2. or 3., but select the donor from the candidates
      with probability that depends on $\left|\hat y_{mis,i} - \hat y_{obs, j}\right|$.\cite{Siddique2008}
\end{enumerate}
\end{frame}


\begin{frame}{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
\blue{Potential issues with donor selection}
\begin{itemize}\itemsep2mm
\item<1-> Selection criteria 2. - 4., require the number of candidates $d$ (or maximal
difference $\eta$) to be specified. Common choices for $d$ are 3, 5 or 10.
\item<2-> If the same donor is chosen in many/all imputations (e.g., because
          only a few similar observed cases are available), the
          \blue{uncertainty about the missing values will be underestimated}.\\[2ex]
\onslide<3->{\blue{\ding{225}} PMM may be \blue{problematic} when
          \begin{itemize}
          \item the \blue{dataset is very small},
          \item the \blue{proportion of missing values is large}, or
          \item one/some \blue{predictor variable(s) are strongly related
                to the missingness}.
          \end{itemize}
}
\item<4-> Therefore, using $d = 1$ (selection criterion 1.) is not a
          good idea. On the other hand, using too many candidates can lead to bad matches.
\item<5-> Schenker and Taylor \cite{Schenker1996} proposed an adaptive procedure
          to select $d$, but it is not used much in practice.
\end{itemize}
\end{frame}


\begin{frame}{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
For the \blue{sampling of the parameters} (step 1 on slide \ref{pmmalgo}),
different approaches have been introduced in the literature:

\bigskip

\begin{tabular}{lp{11cm}}
Type-0 & point estimates $\hat\beta$ are used in both prediction models
         (least squares or maximum likelihood)\\[2ex]
Type-I & $\hat\beta$ to predict $\hat y_{obs}$; $\tilde\beta$ to predict $\hat y_{mis}$
         is sampled from the posterior distribution of $\beta$ (Bayesian) or
         bootstraped\\[2ex]
Type-II & $\tilde\beta$ to predict $\hat y_{obs}$ as well as $\hat y_{mis}$\\[2ex]
Type-III & different draws $\tilde\beta^{(1)}$ and $\tilde\beta^{(2)}$ to
           predict $\hat y_{obs}$ and $\hat y_{mis}$, respectively
\end{tabular}

\bigskip

The use of point estimates (Type-0 and Type-I matching) \blue{underestimates the
uncertainty} about the regression parameters.
\end{frame}

\begin{frame}{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
Another point of consideration is the \blue{choice of the set of data used to train
the prediction models}.

\bigskip

In the version presented on slide \ref{pmmalgo}, the same set of data (all cases
with observed $y$) is used to train the model and to produce predicted values
of $y_{obs}$.

\bigskip

The predictive model will likely fit the observed cases better than the missing cases,
and, hence, \blue{variation will be underestimated}.

\bigskip

As an alternative, the \blue{model could be trained on the whole data}
(using previously imputed values) or to use a \blue{leave-one-out approach} on the
observed data.
\end{frame}


\subsection{What is implemented in software?}
\begin{frame}{\thesection. \insertsection}
\framesubtitle{\thesection.\thesubsection. \insertsubsection}
\blue{mice (in R):}
\begin{itemize}
\item \blue{PMM} via \Rfct{mice.impute.pmm}
      \begin{itemize}
      \item specification of number of donors $d$ (same for all variables)
      \item Type-0, Type-I, Type-II matching
      \end{itemize}
\item \blue{PMM} via \Rfct{mice.impute.midastouch}
      \begin{itemize}
      \item allows leave-one-out estimation of the parameters
      \item distance based donor selection
      \item Type-0, Type-I, Type-II matching
      \end{itemize}
\item \blue{bootstrap} linear regression via \Rfct{mice.impute.norm.boot}
\item \blue{bootstrap} logistic regression via \Rfct{mice.impute.logreg.boot}
\item \blue{Bayesian} linear regression via \Rfct{mice.impute.norm}
\item \ldots
\end{itemize}
%
% \blue{SPSS}
% \begin{itemize}
% \item \blue{PMM} with $d = 1$
% \item linear regression
% \end{itemize}

\end{frame}


\mode<article>{
\section*{Summary of Part I}
}
\begin{frame}[allowframebreaks]{Summary of Part I}
\blue{1. What is Multiple Imputation?}\\
\begin{itemize}
\item Rubin's two ideas:
      \begin{itemize}
      \item Missing values need to be represented by multiple imputed values.
      \item A model is necessary to obain good imputations.
      \end{itemize}
\item Imputed values are obtained from the predictive distribution of the missing data,
given the observed data.
\item Multiple completed datasets are created from the muliple imputed values.
\item Multiple imputation has three steps: Imputation, analysis, pooling
\end{itemize}

\framebreak

\blue{2. Imputation step}
\begin{itemize}
\item Two sources of variation need to be taken into account
      \begin{itemize}
      \item parameter uncertainty
      \item random variation
      \end{itemize}
\item For two main approaches to MI for imputation of non-monotone multivariate missing data
      \begin{itemize}
      \item MICE/FCS
      \item Joint model imputation
      \end{itemize}
\item The MICE algorithm re-uses univariate imputation models by iterating
      through all incomplete variables, multiple times (iterations)
\item Multiple runs through the algorithm are necessary to create multiple
      imputed dataset
\item The convergence of the chains needs to be checked.
\end{itemize}

\framebreak

\blue{3. Analysis step}
\begin{itemize}
\item Analyse each imputed dataset the way you would analyse a complete dataset
\end{itemize}


\blue{4. Pooling}
\begin{itemize}
\item Results from analyses of multiple imputed datasets can be summarized
      by taking the average of the regression coefficients
\item For the total variance, two sources of variation need to be considered:
      \begin{itemize}
      \item within imputation variance
      \item between imputation variance
      \end{itemize}
\end{itemize}

\framebreak

\blue{5. A closer look at the imputation step}
\begin{itemize}
\item Two parametric approaches for imputation:
      \begin{itemize}
      \item Bayesian (sample from posterior distribution of parameters)
      \item Bootstrap (uses bootstrap samples of the data to estimate parameters)
      \end{itemize}
\item Predictive mean matching is a semi-parametric alternative\\
      (it matches observed and missing cases based on their predicted values).
\item In PMM we need to consider
      \begin{itemize}
      \item donor selection
      \item matching type (how parameters are sampled/estimated),
      \item the set of data used to calculate/estmate the parameters.
      \end{itemize}
\item Bayesian and bootstrap imputation take into account the variation, while
      many choices in PMM lead to underestimation of the variation.
\end{itemize}
\end{frame}
