---
title: "EP16: Missing Values in Clinical Research: Multiple Imputation"
subtitle: "12. Imputation with Longitudinal Data"
author: "Nicole Erler"
institute: "Department of Biostatistics, Erasmus Medical Center"
date: ""
email: "n.erler@erasmusmc.nl"
output:
  beamer_presentation:
    keep_tex: false
    template: mytemplate.latex
    includes:
      in_header: [SlideTemplate.tex, defs.tex]
    incremental: false
classoption: [aspectratio=169]
bibliography: references.bib
---

```{r setup, include = FALSE}
projdir <- gsub("/Slides", "", getwd())

runimps <- FALSE
  
knitr::knit_hooks$set(
  nospace = function(before, options, envir) {
    if (before) {
      knitr::asis_output("\\vspace*{-1.5ex}")
    }
  }
)


knitr::opts_chunk$set(echo = TRUE, nospace = TRUE, nospaceafter = TRUE,
                      fig.align = 'center', out.width = "100%")

options(width = 120)

suppressPackageStartupMessages(library("mice"))

library(kableExtra)
library(ggplot2)
library(lme4)

load(file.path(projdir, "Slides/workspaces", "longDF2.RData"))
# knitr::read_chunk(file.path(projdir, "Slides/Rfcts/Section12_Imputations.R"))

load(file.path(projdir, "Slides/workspaces/mice_long.RData"))
load(file.path(projdir, "Slides/workspaces/JointAI_long.RData"))
```

## The Challenge with Longitudinal Data
In \blue{long format}:\vspace*{-1ex}

* (potential) correlation between repeated measurements ignored
* inconsistent imputation of baseline covariates

\medskip\pause

In \blue{wide format}:\vspace*{-1ex}

* for \blue{unbalanced} data: direct wide format may not be possible
* for relatively \blue{balanced} data: inefficient

\medskip\pause

\blue{Simple summaries} to allow wide format:\vspace*{-1ex}

* loss of information
* potential MNAR
* \blue{bias}

## Imputation with mice
\textbf{mice} has functions to allow imputation of longitudinal (2-level) data:
\begin{itemize}
\item \blue{Level 1:}\\
repeated measurements within subjects or subjects within classes
\item \blue{Level 2:}\\
time-constant/baseline covariates, between subjects effects, variables on the group level
\end{itemize}

\bigskip

\begin{columns}[onlytextwidth, T]
\begin{column}{0.5\textwidth}
Imputation methods for \blue{level-1} variables:
\begin{itemize}
\item \Rstring{2l.pan}
\item \Rstring{2l.norm}
\item \Rstring{2l.lmer}
\item \Rstring{2l.bin}
\end{itemize}
\end{column}
\begin{column}{0.5\textwidth}
Imputation methods for \blue{level-2} variables:
\begin{itemize}
\item \Rstring{2lonly.norm}
\item \Rstring{2lonly.pmm}
\item \Rstring{2lonly.mean}
\end{itemize}
\end{column}
\end{columns}

## Imputation with mice
* \Rstring{2l.pan}: linear two-level model with \blue{homogeneous 
  within group variances} [@Schafer2002]
* \Rstring{2l.norm}: (Bayesian) linear two-level model with
  \blue{heterogenous} group variances

\pause

* \Rstring{2l.lmer}/\Rstring{2l.bin}: univariate systematically and sporadically
missing data using a two-level normal/logistic model using \Rfct{lmer}/\Rfct{glmer} from package
\textbf{lme4}.

\pause

* \Rstring{2lonly.norm} and \Rstring{2lonly.pmm}: 
  to impute level-2 variables

\pause 

* \Rstring{2lonly.mean}: imputes values with the mean of the observed values per
  class (only to be used in special cases!)



## Imputation with mice

The \Rarg{predictorMatrix} contains extra info for multi-level imputation:
\begin{itemize}
\item grouping/ID variable: -2
\item random effects (also included as fixed effects): 2
\item fixed effects of group means: 3
\item fixed effects of group means \& random effects: 4
\end{itemize}

\bigskip

In all cases, the group identifier ("id" variable) needs to be set to -2 in
the \Rarg{predictorMatrix}.


## Imputation with mice
\blue{Alternative approach:} [@Erler2016]\
Get a \blue{better summary} of the longitudinal variables!\pause

\includegraphics[width = 0.48\linewidth]{graphics/Section09/plong2_4.pdf}\pause
\includegraphics[width = 0.48\linewidth]{graphics/Section09/plong2_5.pdf}\pause

\blue{Approximate trajectories using random effects!}


## Imputation with mice
> - Fit a flexible mixed model for outcome and other longitudinal variables\
    \blue{\ding{225}} extract the random effects

> - reduce data to a "baseline" version & add random effects as columns

> - impute with **mice**

> - extract imputed data & merge with long. variables  
   
> - convert back to `mids` object & analyse

\pause\bigskip

\blue{Drawback:} cannot handle \blue{incomplete longitudinal variables}.


```{r, eval = runimps, include = FALSE}
mod <- lmer(y ~ ns(time, df = 2) + (ns(time, df = 2) | id), data = longDF2)

bs <- ranef(mod)$id
names(bs) <- paste0('b', 0:2)

basedat <- cbind(longDF2[match(unique(longDF2$id), longDF2$id), ], bs)
                 
imp0 <- mice(basedat, maxit = 0)

meth <- imp0$method
pred <- imp0$predictorMatrix
pred[, c('id', 'time', 'ti', 'y')] <- 0

implong <- mice(basedat, meth = meth, predictorMatrix = pred, maxit = 20)

long <- complete(implong, 'long', include = FALSE)

datlist <- lapply(split(long, long$.imp), function(x) {
       merge(subset(x, select = c(.id, .imp, id, x1, x2, x3, x4)),
             subset(longDF2, select = c(id, time, y)), all = TRUE)
})

newmids <- miceadds::datalist2mids(datlist)


impnaive <- mice(longDF2, maxit = 10)


save(newmids, impnaive, file = file.path(projdir, 'Slides/workspaces/mice_long.RData'))

```



## Imputation with JointAI
Example data:
\begin{itemize}
\item $x1$ (complete)
\item $x2$ (binary, 30\% NA)
\item $x3$ (3 categories, 30\% NA)
\item $x4$ (continuous/normal, 30\% NA)
\item $y$ (longitudinal outcome)
\item $time$ (time variable with quadratic effect)
\item $id$ (id variable)
\end{itemize}


## Imputation with JointAI
The syntax for analysing mixed models in **JointAI** is analogous the syntax
used in \Rfct{lme} of the package \textbf{nlme}.

\small
```{r JointAI_long, eval = runimps}
library("JointAI")
JointAI_long <- lme_imp(y ~ x1 + x2 + x3 + x4 + time + I(time^2),
                        random = ~time|id, data = longDF2,
                        n.iter = 5000)
```
\normalsize

\pause

Again, convergence of the Gibbs sampler needs to be checked before obtaining the results.

Contrary to the two-level imputation of \textbf{mice}, non-linear associations
are appropriately handled.


```{r JointAI_long_run, eval = runimps, echo = FALSE}
save(JointAI_long, file = "Slides/workspaces/JointAI_long.RData")
```

```{r JointAI_long_get, echo = FALSE, fig.width = 8, fig.height = 4, message = FALSE}
res_JointAI_long <- summary(JointAI_long)
```


## Comparison of Results

```{r echo = F, fig.width = 6.5, fig.height = 3.7}
mod_true <- with(longDF2_orig,
                 lmer(y ~ x1 + x2 + x3 + x4 + time + I(time^2) + (time|id)))

micelong <- with(newmids, lmer(y ~ x1 + x2 + x3 + x4 + time + I(time^2) + (time|id)))
res_micelong <- summary(pool(micelong), conf.int = TRUE)

micelong_naive <- with(impnaive,
                       lmer(y ~ x1 + x2 + x3 + x4 + time + I(time^2) + 
                              (time|id), 
                            control = lmerControl(optimizer = 'bobyqa')))
res_micelong_naive <- summary(pool(micelong_naive), conf.int = TRUE)

res_JointAI <- cbind(term = names(coef(JointAI_long)),
                     as.data.frame(res_JointAI_long$stat[names(coef(JointAI_long)),
                                                         c(1,3,4)]))

names(res_JointAI) <- gsub('Mean', 'estimate', names(res_JointAI)) %>%
  gsub('5%', '5 %', .)

res_long2 <- list(JointAI = res_JointAI,
                  mice_long = res_micelong[, c('term', "estimate", "2.5 %", "97.5 %")],
                  mice_naive = res_micelong_naive[, c('term', "estimate", "2.5 %", "97.5 %")]
)


plot_long <- reshape2::melt(res_long2, id.vars = names(res_long2[[1]]))

polyDF <- data.frame(x = rep(c(0.5, length(res_long2) + 0.5)[c(1,2,2,1)], 8),
                     y = c(apply(confint(mod_true,
                                         method = "Wald",
                           parm = names(fixef(mod_true))), 1, rep,
                           each = 2)),
                     term = rep(names(fixef(mod_true)), each = 4)
)

ggplot(plot_long, aes(x = L1, y = estimate)) +
  geom_point(na.rm = T) +
  geom_errorbar(aes(ymin = `2.5 %`, ymax = `97.5 %`), width = 0.3, na.rm = T) +
  facet_wrap("term", scales = 'free', ncol = 4 ) +
  geom_polygon(data = polyDF, aes(x = pmin(x, 2.5), y = y), fill = 'blue', alpha = 0.2) +
  geom_hline(data = data.frame(betas = fixef(mod_true),
                               term = names(fixef(mod_true))),
             aes(yintercept = betas), lty = 2) +
  ylab("coefficient") +
  xlab("") +
   theme(panel.background = element_rect(fill = grey(0.98), color = grey(0.85)),
          panel.grid = element_blank(),
          legend.position = 'none',
          legend.key.width = unit(10, "mm"),
          legend.key.height = unit(10, "mm"),
         axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_x_discrete(limits = c("mice_long", "JointAI"),
                   labels = c("mice", "JointAI"))
  
```
\vspace*{-3ex}

## Comparison of Results

```{r echo = F, fig.width = 6.5, fig.height = 3.7}
ggplot(plot_long, aes(x = L1, y = estimate)) +
  geom_point(na.rm = T) +
  geom_errorbar(aes(ymin = `2.5 %`, ymax = `97.5 %`), width = 0.3, na.rm = T) +
  facet_wrap("term", scales = 'free', ncol = 4 ) +
  geom_polygon(data = polyDF, aes(x = x, y = y), fill = 'blue', alpha = 0.2) +
  geom_hline(data = data.frame(betas = fixef(mod_true),
                               term = names(fixef(mod_true))),
             aes(yintercept = betas), lty = 2) +
  ylab("coefficient") +
  xlab("") +
   theme(panel.background = element_rect(fill = grey(0.98), color = grey(0.85)),
          panel.grid = element_blank(),
          legend.position = 'none',
          legend.key.width = unit(10, "mm"),
          legend.key.height = unit(10, "mm"),
         axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_x_discrete(limits = c("mice_naive", "mice_long", "JointAI"),
                   labels = c("naive", "mice", "JointAI"))
``` 
\vspace*{-3ex}


## References
